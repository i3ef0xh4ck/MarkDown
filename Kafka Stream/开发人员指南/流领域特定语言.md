[原文地址](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)

# 流领域特定语言
Kafka Streams DSL（域特定语言）构建在Streams Processor API之上。 这是大多数用户推荐的，特别是初学者。 大多数数据处理操作都可以用几行DSL代码来表示。
## 概述
与处理器API相比，只有DSL支持：
- 内置KStream，KTable和GlobalKTable形式的流和表的抽象。 拥有流和表的一流支持是至关重要的，因为在实践中，大多数用例不仅需要流或数据库/表，而且还需要两者的组合。 例如，如果您的用例是创建实时更新的客户360度视图，那么您的应用程序将会将许多与客户相关的事件输入流转换为包含不断更新的360度视图的输出表 客户的观点。
- 具有[无状态变换](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-transformations-stateless)（例如map和filter）以及诸如聚集（例如count:计数和reduce:减少），连接（例如，leftJoin:左连接）和窗口化（例如session window:会话窗口）的[有状态变换](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-transformations-stateful)的声明性，函数式编程风格。

使用DSL，您可以在应用程序中定义处理器拓扑（即逻辑处理计划）。 完成这个步骤是：
1. 指定[一个或多个从Kafka主题读取的输入流](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-sources)。
2. 在这些流上组合[转换](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-transformations)。
3. 将[结果输出流写回到Kafka主题](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-destinations)，或通过[交互式查询](https://kafka.apache.org/10/documentation/streams/developer-guide/interactive-queries.html#streams-developer-guide-interactive-queries)（例如，通过REST API）将应用程序的处理结果直接公开给其他应用程序。

在应用程序运行之后，定义的处理器拓扑结构被连续地执行（即处理计划付诸实施）。 下面提供了使用DSL编写流处理应用程序的分步指南。

有关可用API功能的完整列表，请参阅[Kafka Streams Javadocs](https://kafka.apache.org/10/documentation/streams/javadocs.html#streams-javadocs)。

## 创建来自KAFKA的源流
您可以轻松地从Kafka主题中读取数据到您的应用程序中。 支持以下操作。

从Kafka读取 | 描述
---- | ---
Stream  <br /><br />  input topics → KStream| 从指定的Kafka输入主题创建KStream，并将数据解释为记录流。 KStream表示分区的记录流。 （细节）<br/><br />在KStream的情况下，每个应用程序实例的本地KStream实例将仅填充来自输入主题的一部分分区的数据。 总而言之，在所有应用程序实例中，读取和处理所有输入主题分区。<br /><br />[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br /><br />如果您没有明确指定SerDes，则使用配置中的默认SerDes。 <br /><br />如果Kafka输入主题中的记录的键或值类型与配置的默认SerDes不匹配，则必须明确指定SerDes。 有关配置默认SerDes，可用SerDes和实现您自己的自定义SerDes的信息，请参阅数据类型和序列化。 <br /><br /> 存在多个流的变体，例如，为输入主题指定正则表达式模式。
Table <br /><br /> input topic → KTable|将指定的Kafka输入主题读入KTable。该主题被解释为更改日志流，其中具有相同键的记录被解释为对于该键的UPSERT aka INSERT / UPDATE（当记录值不为空时）或DELETE（当值为空时）。 （细节）<br /><br />在KStream的情况下，每个应用程序实例的本地KStream实例将仅填充来自输入主题的一部分分区的数据。总而言之，在所有应用程序实例中，读取和处理所有输入主题分区。<br /><br />您必须为该表提供一个名称（更确切地说，对于支持该表的内部状态存储）。这对于支持对表的交互式查询是必需的。如果没有提供名称，表格将不会被查询，并且会为状态存储提供一个内部名称。<br /><br />如果您没有明确指定SerDes，则使用配置中的默认SerDes。<br /><br />如果Kafka输入主题中的记录的键或值类型与配置的默认SerDes不匹配，则必须明确指定SerDes。有关配置默认SerDes，可用SerDes和实现您自己的自定义SerDes的信息，请参阅[数据类型和序列化](https://kafka.apache.org/10/documentation/streams/developer-guide/datatypes.html#streams-developer-guide-serdes)。<br /><br />存在多种表格变体，例如指定从输入主题读取时要使用的auto.offset.reset策略。
Global Table<br /><br />input topic → GlobalKTable|将指定的Kafka输入主题读取到GlobalKTable中。 该主题被解释为更改日志流，其中具有相同键的记录被解释为对于该键的UPSERT aka INSERT / UPDATE（当记录值不为空时）或DELETE（当值为空时）。 （细节）<br /><br />在GlobalKTable的情况下，每个应用程序实例的本地GlobalKTable实例将仅填充来自输入主题的一部分分区的数据。 总而言之，在所有应用程序实例中，读取和处理所有输入主题分区。<br /><br />您必须为该表提供一个名称（更确切地说，对于支持该表的内部状态存储）。 这对于支持对表的交互式查询是必需的。 如果没有提供名称，表格将不会被查询，并且会为状态存储提供一个内部名称。<br /><br />[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br /><br />如果Kafka输入主题中的记录的键或值类型与配置的默认SerDes不匹配，则必须明确指定SerDes。 有关配置默认SerDes，可用SerDes和实现您自己的自定义SerDes的信息，请参阅数据类型和序列化。<br /><br />globalTable的几个变体存在于例如 指定显式SerDes。

## 流转换
KStream和KTable接口支持各种转换操作。 这些操作中的每一个都可以被转换成一个或多个连接的处理器到底层的处理器拓扑中。 由于KStream和KTable是强类型的，所有这些转换操作都被定义为通用函数，用户可以指定输入和输出数据类型。

一些KStream转换可能会生成一个或多个KStream对象，例如： - 过滤和映射KStream会生成另一个KStream - KStream上的分支可以生成多个KStream

其他一些可能会生成一个KTable对象，例如一个KStream的聚合也产生一个KTable。 这允许Kafka Streams在已经产生给下游转化操作员后的迟到记录到达时连续更新计算值。

所有KTable转换操作只能生成另一个KTable。 但是，Kafka Streams DSL确实提供了将KTable表示转换为KStream的特殊功能。 所有这些转换方法可以链接在一起组成一个复杂的处理器拓扑。

这些转换操作在以下小节中进行介绍：
- 无状态转换
- 有状态转换

### 无状态转换
无状态转换不需要状态进行处理，也不需要与流处理器相关联的状态存储。 Kafka 0.11.0和更高版本允许您实现无状态KTable转换的结果。 这允许通过交互式查询来查询结果。 为了实现KTable，下面的每个无状态操作都可以用一个可选的queryableStoreName参数来扩充。

转换|描述
----|---
分支<br /><br/>KStream → KStream[]|将基于提供的断言的KStream分支（或拆分）为一个或多个KStream实例。 （详情）<br /><br />断言按顺序进行评估。 记录被放置到第一个匹配的唯一一个输出流：如果第n个断言的计算结果为true，则记录被放置到第n个流。 如果没有断言匹配，记录将被删除。<br /><br />分支很有用，例如，可用于将记录路由到不同的下游主题。<br /><br />[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br /><br />
过滤<br><br>KStream → KStream<br><br>KTable → KTable|`stream.filter`为每个元素计算布尔函数，并保留函数返回true的那些元素。 （KStream细节，KTable细节） <br /><br />[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br /><br />
相反过滤器<br><br>KStream → KStream<br><br>KTable → KTable|`stream.filterNot`为每个元素计算布尔函数，并删除函数返回true的布尔函数。 （KStream细节，KTable细节） <br /><br />[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br /><br />
FlatMap<br/><br/>KStream → KStream|记录一条记录并生成零个，一个或多个记录。 您可以修改记录键和值，包括它们的类型。 （细节）<br/><br/>将数据流标记为数据重新分区：在flatMap之后应用分组或连接将导致重新分区记录。 如果可能，请使用flatMapValues，而不会导致数据重新分区。 <br /><br />[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br /><br />
FlatMap (values only)<br><br>KStream → KStream|记录一条记录并生成零个，一个或多个记录，同时保留原始记录的键。 您可以修改记录值和值类型。 （细节）<br><br>flatMapValues比flatMap更好，因为它不会导致数据重新分区。 但是，您不能像flatMap那样修改键或键类型。<br /><br />[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)
Foreach <br><br>KStream → void<br><br>KStream → void<br><br>KTable → void|终端操作。 对每条记录执行无状态操作。 （细节）<br><br>你会使用foreach引起基于输入数据的副作用（类似于peek），然后停止对输入数据的进一步处理（不像peek，这不是终端操作）。<br><br>关于处理保证的注意事项：Kafka 无法追踪任何行为的副作用（如写入外部系统），这意味着他们通常不会从Kafka 的处理保证中受益。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)
GroupByKey<br><br>KStream → KGroupedStream|按现有键对记录进行分组<br><br>分组是聚合流或表的先决条件，并确保数据被正确分区（“键入”）以供后续操作使用。<br><br>何时设置显式SerDes：存在groupByKey的变量以覆盖应用程序的已配置默认SerDes，如果生成的KGroupedStream的键和/或值类型与所配置的默认SerDes不匹配，则必须执行此操作。<br><br>注意<br><br>分组与窗口化：相关的操作是窗口化，它允许您控制如何将同一个键的分组记录“分组”到用于有状态操作（例如窗口化聚合或窗口化联接）的所谓窗口中。<br><br>导致数据重新分区当且仅当流标记为重新分区。 groupByKey优于groupBy，因为只有当流已经被标记为重新分区时，它才会重新分区数据。 但是，groupByKey不允许您修改像groupBy那样的键或键类型。。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)
GroupBy<br><br>KStream → KGroupedStream<br><br>KTable → KGroupedTable|通过一个新的键分组记录，这可能是一个不同的键类型。 表分组时，您也可以指定一个新的值和值类型。 groupBy是selectKey（...）.groupByKey（）的简写。 （KStream细节，KTable细节）<br><br>分组是聚合流或表的先决条件，并确保数据被正确分区（“键入”）以供后续操作使用。<br><br>何时设置显式SerDes：存在groupBy的变体以覆盖应用程序的已配置默认SerDes，如果生成的KGroupedStream或KGroupedTable的键和/或值类型与已配置的默认SerDes不匹配，则必须执行此操作。<br><br>注意<br><br>分组与窗口化：窗口化操作，它允许您控制如何将同一个键的分组记录“分组”到用于有状态操作（例如窗口化聚合或窗口化联接）的所谓窗口中。<br><br>始终导致数据重新分区：groupBy总是导致数据重新分区。 如果可能，请使用groupByKey，只有在需要时才会重新分区数据。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)
Map<br><br>KStream → KStream|取一条记录并产生一条记录。 您可以修改记录键和值，包括它们的类型。 （细节）<br><br>将流标记为数据重新分区：在映射之后应用分组或连接将导致重新分区记录。 如果可能的话，使用mapValues，这不会导致数据重新分区。<br><br>`stream.map`<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)
Map (values only)<br><br>KStream → KStream<br><br>KTable → KTable|取一条记录并产生一条记录，同时保留原始记录的键。 您可以修改记录值和值类型。 （KStream细节，KTable细节）<br><br>mapValues比map更好，因为它不会导致数据重新分区。 但是，它不允许您像`map`一样修改键或键类型。
Peek<br><br>KStream → KStream|对每条记录执行无状态操作，并返回未更改的流。 （细节）<br><br>你会使用`peek`导致基于输入数据（类似于foreach）的副作用，并继续处理输入数据（不像foreach，这是一个终端操作）。 peek按原样返回输入流; 如果您需要修改输入流，请改用map或mapValues。<br><br>对于诸如日志记录或跟踪指标或用于调试和故障排除的用例，peek很有帮助。<br><br>关于处理保证的注意事项：卡夫卡无法追踪任何行为的副作用（如写入外部系统），这意味着他们通常不会从卡夫卡的处理保证中受益。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)
Print<br><br>KStream → void|终端操作。 将记录打印到System.out。 请参阅Javadocs的serde和toString（）注意事项。 （细节）<br><br>调用print（）和调用foreach（（key，value） - > System.out.println（key +“，”+ value））是一样的<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)
SelectKey<br><br>KStream → KStream|为每个记录分配一个新的键（可能是一个新的键类型）。 （细节）<br><br>调用selectKey（mapper）与调用map（（key，value） - > mapper（key，value），value）相同。<br><br>将数据流标记为数据重新分区：在selectKey之后应用分组或联接将导致重新分区记录。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)
Table to Stream<br><br>KTable → KStream|获取此表的更新日志流。 （细节）<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)

### 有状态转换
状态转换依赖于处理输入和产生输出的状态，并且需要与流处理器相关联的状态存储。 例如，在聚合操作中，使用窗口状态存储来收集每个窗口的最新聚合结果。 在连接操作中，使用窗口状态存储来收集迄今为止在定义的窗口边界内收到的所有记录。

请注意，该状态存储是容错的。 如果发生故障，Kafka Streams保证在恢复处理之前完全恢复所有状态存储。 有关更多信息，请参阅[容错](https://kafka.apache.org/10/documentation/streams/architecture.html#streams-architecture-fault-tolerance)。

DSL中可用的有状态转换包括：
- [聚合](#聚合)（Aggregating）
- [连接](#连接)（Joining）
- [窗口化](#窗口化)（Windowing (as part of aggregations and joins)）
- [使用有状态集成处理器API的自定义处理器和转换器](#使用有状态集成处理器API的自定义处理器和转换器)

关系如下：
![streams-stateful_operations.png](../streams-stateful_operations.png)

DSL中的状态转换

以下是有状态应用程序的示例：WordCount算法。

Java 8+中的WordCount示例，使用lambda表达式：
```
// Assume the record values represent lines of text.  For the sake of this example, you can ignore
// whatever may be stored in the record keys.
KStream<String, String> textLines = ...;

KStream<String, Long> wordCounts = textLines
    // Split each text line, by whitespace, into words.  The text lines are the record
    // values, i.e. you can ignore whatever data is in the record keys and thus invoke
    // `flatMapValues` instead of the more generic `flatMap`.
    .flatMapValues(value -> Arrays.asList(value.toLowerCase().split("\\W+")))
    // Group the stream by word to ensure the key of the record is the word.
    .groupBy((key, word) -> word)
    // Count the occurrences of each word (record key).
    //
    // This will change the stream type from `KGroupedStream<String, String>` to
    // `KTable<String, Long>` (word -> count).
    .count()
    // Convert the `KTable<String, Long>` into a `KStream<String, Long>`.
    .toStream();
```
WordCount example in Java 7:
```
// Code below is equivalent to the previous Java 8+ example above.
KStream<String, String> textLines = ...;

KStream<String, Long> wordCounts = textLines
    .flatMapValues(new ValueMapper<String, Iterable<String>>() {
        @Override
        public Iterable<String> apply(String value) {
            return Arrays.asList(value.toLowerCase().split("\\W+"));
        }
    })
    .groupBy(new KeyValueMapper<String, String, String>>() {
        @Override
        public String apply(String key, String word) {
            return word;
        }
    })
    .count()
    .toStream();
```

#### <h4 id="聚合">聚合</h4>
在通过groupByKey或groupBy按键对记录进行分组之后，可以将其表示为KGroupedStream或KGroupedTable，然后通过reduce等操作进行聚合。 聚合是基于键的操作，这意味着它们总是对相同键的记录（特别是记录值）进行操作。 您可以在窗口或非窗口数据上执行聚合。

转换|描述
----|---
聚合<br><br>KGroupedStream → KTable<br><br>KGroupedTable → KTable|滚动聚合。 通过分组键聚合（非窗口）记录的值。 聚合是reduce的泛化，例如，允许聚合值具有与输入值不同的类型。 （KGroupedStream细节，KGroupedTable细节）<br><br>聚合分组流时，您必须提供初始值（注：initializer 初始化器，可以是new方法也可以是具体的值，例如，aggValue = 0）和“加法器”聚合器（例如，aggValue + curValue）。 聚合分组表时，您必须提供一个“减法器”聚合器（想想：aggValue - oldValue）。<br><br>有几个聚合变体存在，请参阅Javadocs的细节。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>KGroupedStream的详细行为：<br><br>用空键输入记录将被忽略。<br><br>当第一次接收到记录键时，初始化程序被调用（并在加法器之前调用）。<br><br>每当收到一个非空值的记录时，就调用加法器。<br><br>KGroupedTable详细的行为：<br><br>用空键输入记录将被忽略。<br><br>当第一次接收到记录密钥时，初始化程序被调用（并在加法器和减法器之前调用）。请注意，与KGroupedStream相比，随着时间的推移，初始化程序可能会因为接收到该键的输入逻辑删除记录（参见下文）而被重复调用一次以上。<br><br>当接收到第一个非空值（例如，INSERT）时，则仅调用加法器。<br><br>当接收到关键字（例如，UPDATE）的后续非空值时，则（1）使用存储在表中的旧值调用减法器，以及（2）用输入记录的新值调用加法器刚刚收到。没有定义减法器和加法器的执行顺序。<br><br>当一个逻辑删除记录（即具有空值的记录）被接收到一个密钥（例如DELETE）时，只有减法器被调用。请注意，只要减法器本身返回一个空值，那么相应的键将从生成的KTable中移除。如果发生这种情况，该键的任何下一个输入记录将再次触发初始化程序。<br><br>请参阅本节底部的示例，了解聚合语义的可视化。
Aggregate (windowed)<br><br>KGroupedStream → KTable|窗口聚合。 按照分组键，汇总每个窗口的记录值。 聚集是reduce的泛化，例如，允许聚合值具有与输入值不同的类型。（TimeWindowedKStream细节，SessionWindowedKStream细节）<br><br>您必须提供一个初始化程序（例如，aggValue = 0），“加法器”聚合器（例如，aggValue + curValue）和一个窗口。 当基于会话进行窗口化时，还必须提供“会话合并”聚合器（例如，mergedAggValue = leftAggValue + rightAggValue）。<br><br>窗口化聚合将TimeWindowedKStream <K，V>或SessionWindowdKStream <K，V>转换为窗口化的KTable <Windowed <K>，V>。<br><br>有几个聚合变体存在，请参阅Javadocs的细节。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>开窗的集料表现类似于上述的轧制集料。 额外的麻烦是，行为适用于每个窗口。<br><br>一般情况下，输入的空键记录被忽略。<br><br>当给定窗口首次接收到记录密钥时，调用初始化程序（并在加法器之前调用）。<br><br>每当一个给定的窗口收到一个非空值的记录时，就调用加法器。<br><br>在使用会话窗口时：只要合并两个会话，就会调用会话合并。<br><br>请参阅本节底部的示例，了解聚合语义的可视化。
Count<br><br>KGroupedStream → KTable<br><br>KGroupedTable → KTable|滚动聚合。 通过分组键计算记录数。 （KGroupedStream细节，KGroupedTable细节）<br><br>计数的几个变种，请参阅Javadocs的细节。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>KGroupedStream的详细行为：<br><br>具有空键或值的输入记录被忽略。<br><br><br><br>KGroupedTable的详细行为：<br><br>用空键输入记录将被忽略。 具有空值的记录不会被忽略，而是被解释为相应键的“墓碑”，其指示从表中删除键。
Count (windowed)<br><br>KGroupedStream → KTable|窗口聚合。 通过分组键计算每个窗口的记录数。 （TimeWindowedKStream细节，SessionWindowedKStream细节）<br><br>窗口计数将TimeWindowedKStream <K，V>或SessionWindowedKStream <K，V>转换为窗口化的KTable <Windowed <K>，V>。<br><br>计数的几个变种，请参阅Javadocs的细节。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>具有空键或值的输入记录被忽略。
Reduce<br><br>KGroupedStream → KTable<br><br>KGroupedTable → KTable|滚动聚合。 通过分组键组合（非窗口）记录的值。 当前记录值与最后一个减小的值结合，并返回一个新的减少的值。 不像聚合，结果值类型不能改变。 （KGroupedStream细节，KGroupedTable细节）<br><br>减少分组流时，必须提供“加法器”减速器（例如，aggValue + curValue）。 减少分组表时，还必须提供一个“减法器”减速器（例如，aggValue - oldValue）。<br><br>减少的几种变种，请参阅Javadocs的细节。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>KGroupedStream的详细行为：<br><br>一般情况下，输入的空键记录被忽略。<br><br>当第一次接收到一个记录键时，那个记录的值被用作初始聚合值。<br><br>每当收到一个非空值的记录时，就调用加法器。<br><br>KGroupedTable的详细行为：<br><br>一般情况下，输入的空键记录被忽略。<br><br>当第一次接收到一个记录键时，那个记录的值被用作初始聚合值。请注意，与KGroupedStream相反，随着时间的推移，由于接收到该键的输入逻辑删除记录（见下文），此初始化步骤可能会多次发生。<br><br>当接收到第一个非空值（例如，INSERT）时，则仅调用加法器。<br><br>当接收到关键字（例如，UPDATE）的后续非空值时，则（1）使用存储在表中的旧值调用减法器，以及（2）用输入记录的新值调用加法器刚刚收到。没有定义减法器和加法器的执行顺序。<br><br>当一个逻辑删除记录（即具有空值的记录）被接收到一个密钥（例如DELETE）时，只有减法器被调用。请注意，只要减法器本身返回一个空值，那么相应的键将从生成的KTable中移除。如果发生这种情况，该键的任何下一个输入记录将重新初始化其总值。<br><br>请参阅本节底部的示例，了解聚合语义的可视化。
Reduce (windowed)<br><br>KGroupedStream → KTable|窗口聚合。 通过组合键将每个窗口的记录值组合在一起。 当前记录值与最后一个减小的值结合，并返回一个新的减少的值。 空键或值的记录被忽略。 不像聚合，结果值类型不能改变。 （TimeWindowedKStream细节，SessionWindowedKStream细节）<br><br>窗口化缩小将TimeWindowedKStream <K，V>或SessionWindowedKStream <K，V>变成窗口化的KTable <Windowed <K>，V>。<br><br>减少的几种变种，请参阅Javadocs的细节。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>窗口减少表现与上述的滚动减少类似。 额外的麻烦是，行为适用于每个窗口。<br><br>一般情况下，输入的空键记录被忽略。<br><br>当给定窗口首次接收到记录密钥时，该记录的值将用作初始聚合值。<br><br>每当一个给定的窗口收到一个非空值的记录时，就调用加法器。<br><br>请参阅本节底部的示例，了解聚合语义的可视化。

流聚合的语义示例：KGroupedStream→KTable示例如下所示。 流和表最初是空的。 粗体字体在“KTable聚合”列中用于突出显示已更改的状态。 诸如（hello，1）之类的条目表示具有关键字hello和值1的记录。为了提高语义表的可读性，可以假定所有记录按时间戳顺序处理。
```
// Key: word, value: count
KStream<String, Integer> wordCounts = ...;

KGroupedStream<String, Integer> groupedStream = wordCounts
    .groupByKey(Serialized.with(Serdes.String(), Serdes.Integer()));

KTable<String, Integer> aggregated = groupedStream.aggregate(
    () -> 0, /* initializer */
    (aggKey, newValue, aggValue) -> aggValue + newValue, /* adder */
    Materialized.<String, Long, KeyValueStore<Bytes, byte[]>as("aggregated-stream-store" /* state store name */)
      .withKeySerde(Serdes.String()) /* key serde */
      .withValueSerde(Serdes.Integer()); /* serde for aggregate value */
```
注意

记录缓存的影响：出于说明的目的，下面的“KTable聚合”列以非常详细的方式显示了表的状态随时间的变化。 在实践中，只有在记录缓存被禁用的情况下（默认：启用），才能以这种细粒度的方式观察状态更改。 当启用记录缓存时，会发生什么情况，例如，时间戳4和5的行的输出结果将被压缩，并且KTable中的键kafka将只有一个状态更新（这里是：from（kafka 1）直接指向（kafka，3）。通常情况下，只应该禁用记录缓存才能进行测试或调试 - 在正常情况下，最好将记录缓存启用。
<table>
  <tr>
    <td></td>
    <td colspan="2">KStream `wordCounts`</td>
    <td colspan="2">KGroupedStream `groupedStream`</td>
    <td>KTable `aggregated`</td>
  </tr>
  <tr>
    <td >Timestamp</td>
    <td >Input record</td>
    <td >Grouping</td>
    <td >Initializer</td>
    <td >Adder</td>
    <td >State</td>
  </tr>

  <tr class="row-odd"><td>1</td>
      <td>(hello, 1)</td>
      <td>(hello, 1)</td>
      <td>0 (for hello)</td>
      <td>(hello, 0 + 1)</td>
      <td><div class="first last line-block">
          <div class="line"><strong>(hello, 1)</strong></div>
      </div>
      </td>
  </tr>
  <tr class="row-even"><td>2</td>
      <td>(kafka, 1)</td>
      <td>(kafka, 1)</td>
      <td>0 (for kafka)</td>
      <td>(kafka, 0 + 1)</td>
      <td><div class="first last line-block">
          <div class="line">(hello, 1)</div>
          <div class="line"><strong>(kafka, 1)</strong></div>
      </div>
      </td>
  </tr>
  <tr class="row-odd"><td>3</td>
      <td>(streams, 1)</td>
      <td>(streams, 1)</td>
      <td>0 (for streams)</td>
      <td>(streams, 0 + 1)</td>
      <td><div class="first last line-block">
          <div class="line">(hello, 1)</div>
          <div class="line">(kafka, 1)</div>
          <div class="line"><strong>(streams, 1)</strong></div>
      </div>
      </td>
  </tr>
  <tr class="row-even"><td>4</td>
      <td>(kafka, 1)</td>
      <td>(kafka, 1)</td>
      <td>&nbsp;</td>
      <td>(kafka, 1 + 1)</td>
      <td><div class="first last line-block">
          <div class="line">(hello, 1)</div>
          <div class="line">(kafka, <strong>2</strong>)</div>
          <div class="line">(streams, 1)</div>
      </div>
      </td>
  </tr>
  <tr class="row-odd"><td>5</td>
      <td>(kafka, 1)</td>
      <td>(kafka, 1)</td>
      <td>&nbsp;</td>
      <td>(kafka, 2 + 1)</td>
      <td><div class="first last line-block">
          <div class="line">(hello, 1)</div>
          <div class="line">(kafka, <strong>3</strong>)</div>
          <div class="line">(streams, 1)</div>
      </div>
      </td>
  </tr>
  <tr class="row-even"><td>6</td>
      <td>(streams, 1)</td>
      <td>(streams, 1)</td>
      <td>&nbsp;</td>
      <td>(streams, 1 + 1)</td>
      <td><div class="first last line-block">
          <div class="line">(hello, 1)</div>
          <div class="line">(kafka, 3)</div>
          <div class="line">(streams, <strong>2</strong>)</div>
      </div>
      </td>
  </tr>
</table>

表聚合的语义示例：KGroupedTable→KTable示例如下所示。 表格最初是空的。 粗体字体在“KTable聚合”列中用于突出显示已更改的状态。 诸如（hello，1）之类的条目表示具有关键字hello和值1的记录。为了提高语义表的可读性，可以假定所有记录按时间戳顺序处理。
```
// Key: username, value: user region (abbreviated to "E" for "Europe", "A" for "Asia")
KTable<String, String> userProfiles = ...;

// Re-group `userProfiles`.  Don't read too much into what the grouping does:
// its prime purpose in this example is to show the *effects* of the grouping
// in the subsequent aggregation.
KGroupedTable<String, Integer> groupedTable = userProfiles
    .groupBy((user, region) -> KeyValue.pair(region, user.length()), Serdes.String(), Serdes.Integer());

KTable<String, Integer> aggregated = groupedTable.aggregate(
    () -> 0, /* initializer */
    (aggKey, newValue, aggValue) -> aggValue + newValue, /* adder */
    (aggKey, oldValue, aggValue) -> aggValue - oldValue, /* subtractor */
    Materialized.<String, Long, KeyValueStore<Bytes, byte[]>as("aggregated-table-store" /* state store name */)
      .withKeySerde(Serdes.String()) /* key serde */
      .withValueSerde(Serdes.Integer()); /* serde for aggregate value */
```
注意

记录缓存的影响：出于说明的目的，下面的“KTable聚合”列以非常详细的方式显示了表的状态随时间的变化。 在实践中，只有在记录缓存被禁用的情况下（默认：启用），才能以这种细粒度的方式观察状态更改。 当启用记录缓存时，会发生什么情况，例如，时间戳4和5的行的输出结果将被压缩，并且KTable中的密钥kafka将只有一个状态更新（这里是：from（kafka 1）直接指向（kafka，3）。通常情况下，只应该禁用记录缓存才能进行测试或调试 - 在正常情况下，最好将记录缓存启用。

<table border="1" class="docutils">
  <colgroup>
      <col width="9%">
      <col width="14%">
      <col width="15%">
      <col width="11%">
      <col width="11%">
      <col width="11%">
      <col width="11%">
      <col width="19%">
  </colgroup>
  <thead valign="bottom">
  <tr class="row-odd"><th class="head">&nbsp;</th>
      <th class="head" colspan="3">KTable <code class="docutils literal"><span class="pre">userProfiles</span></code></th>
      <th class="head" colspan="3">KGroupedTable <code class="docutils literal"><span class="pre">groupedTable</span></code></th>
      <th class="head">KTable <code class="docutils literal"><span class="pre">aggregated</span></code></th>
  </tr>
  <tr class="row-even"><th class="head">Timestamp</th>
      <th class="head">Input record</th>
      <th class="head">Interpreted as</th>
      <th class="head">Grouping</th>
      <th class="head">Initializer</th>
      <th class="head">Adder</th>
      <th class="head">Subtractor</th>
      <th class="head">State</th>
  </tr>
  </thead>
  <tbody valign="top">
  <tr class="row-odd"><td>1</td>
      <td>(alice, E)</td>
      <td>INSERT alice</td>
      <td>(E, 5)</td>
      <td>0 (for E)</td>
      <td>(E, 0 + 5)</td>
      <td>&nbsp;</td>
      <td><div class="first last line-block">
          <div class="line"><strong>(E, 5)</strong></div>
      </div>
      </td>
  </tr>
  <tr class="row-even"><td>2</td>
      <td>(bob, A)</td>
      <td>INSERT bob</td>
      <td>(A, 3)</td>
      <td>0 (for A)</td>
      <td>(A, 0 + 3)</td>
      <td>&nbsp;</td>
      <td><div class="first last line-block">
          <div class="line"><strong>(A, 3)</strong></div>
          <div class="line">(E, 5)</div>
      </div>
      </td>
  </tr>
  <tr class="row-odd"><td>3</td>
      <td>(charlie, A)</td>
      <td>INSERT charlie</td>
      <td>(A, 7)</td>
      <td>&nbsp;</td>
      <td>(A, 3 + 7)</td>
      <td>&nbsp;</td>
      <td><div class="first last line-block">
          <div class="line">(A, <strong>10</strong>)</div>
          <div class="line">(E, 5)</div>
      </div>
      </td>
  </tr>
  <tr class="row-even"><td>4</td>
      <td>(alice, A)</td>
      <td>UPDATE alice</td>
      <td>(A, 5)</td>
      <td>&nbsp;</td>
      <td>(A, 10 + 5)</td>
      <td>(E, 5 - 5)</td>
      <td><div class="first last line-block">
          <div class="line">(A, <strong>15</strong>)</div>
          <div class="line">(E, <strong>0</strong>)</div>
      </div>
      </td>
  </tr>
  <tr class="row-odd"><td>5</td>
      <td>(charlie, null)</td>
      <td>DELETE charlie</td>
      <td>(null, 7)</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>(A, 15 - 7)</td>
      <td><div class="first last line-block">
          <div class="line">(A, <strong>8</strong>)</div>
          <div class="line">(E, 0)</div>
      </div>
      </td>
  </tr>
  <tr class="row-even"><td>6</td>
      <td>(null, E)</td>
      <td><em>ignored</em></td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><div class="first last line-block">
          <div class="line">(A, 8)</div>
          <div class="line">(E, 0)</div>
      </div>
      </td>
  </tr>
  <tr class="row-odd"><td>7</td>
      <td>(bob, E)</td>
      <td>UPDATE bob</td>
      <td>(E, 3)</td>
      <td>&nbsp;</td>
      <td>(E, 0 + 3)</td>
      <td>(A, 8 - 3)</td>
      <td><div class="first last line-block">
          <div class="line">(A, <strong>5</strong>)</div>
          <div class="line">(E, <strong>3</strong>)</div>
      </div>
      </td>
  </tr>
  </tbody>
</table>

#### <h4 id="连接">连接</h4>

流和表也可以连接。实际上，许多流处理应用程序被编码为流连接。例如，支持在线商店的应用可能需要访问多个更新数据库表（例如，销售价格，库存，客户信息）以便利用上下文信息丰富新的数据记录（例如，客户交易）。也就是说，您需要以超大规模和低处理延迟执行表查找的情况。在这里，流行的模式是通过所谓的更改数据捕获与Kafka的Connect API结合，使Kafka中的数据库中的信息可用，然后实现利用Streams API执行非常快速和高效的本地连接的应用程序和流，而不是要求应用程序通过网络对每个记录查询远程数据库。在这个例子中，Kafka Streams中的KTable概念将使您能够跟踪本地状态存储中每个表的最新状态（例如快照），从而大大减少处理延迟，并在执行时减少远程数据库的负载这样的流式连接。

支持以下连接操作，另请参见“状态转换”概述部分的图表。取决于操作数，连接可以是窗口连接或非窗口连接。
<table border="1" class="docutils">
  <colgroup>
      <col width="12%">
      <col width="6%">
      <col width="7%">
      <col width="7%">
      <col width="7%">
      <col width="61%">
  </colgroup>
  <thead valign="bottom">
    <tr class="row-odd"><th class="head">连接操作</th>
        <th class="head">Type</th>
        <th class="head">(INNER) JOIN</th>
        <th class="head">LEFT JOIN</th>
        <th class="head">OUTER JOIN</th>
    </tr>
  </thead>
  <tbody valign="top">
    <tr class="row-even"><td>KStream-to-KStream</td>
        <td>Windowed</td>
        <td>支持</td>
        <td>支持</td>
        <td>支持</td>
    </tr>
    <tr class="row-odd"><td>KTable-to-KTable</td>
        <td>Non-windowed</td>
        <td>支持</td>
        <td>支持</td>
        <td>支持</td>
    </tr>
    <tr class="row-even"><td>KStream-to-KTable</td>
        <td>Non-windowed</td>
        <td>支持</td>
        <td>支持</td>
        <td>不支持</td>
    </tr>
    <tr class="row-odd"><td>KStream-to-GlobalKTable</td>
        <td>Non-windowed</td>
        <td>支持</td>
        <td>支持</td>
        <td>不支持</td>
    </tr>
    <tr class="row-even"><td>KTable-to-GlobalKTable</td>
        <td>N/A</td>
        <td>不支持</td>
        <td>不支持</td>
        <td>不支持</td>
        <td>N/A</td>
    </tr>
  </tbody>
</table>
每个案例在后面的章节中有更详细的解释。

##### 连接要求共同的分区
输入数据连接时必须共同分区。 这确保了在加入过程中，来自加入双方的具有相同键的输入记录被传递到相同的流任务。 加入时确保数据共同分区是用户的责任。

提示  
如果可能，请考虑使用全局表（GlobalKTable）进行连接，因为它们不需要数据共分区。  
数据共分区的要求是：
- 连接的输入主题（左侧和右侧）必须具有相同数量的分区。
- 所有写入输入主题的应用程序都必须具有相同的分区策略，以便将具有相同键的记录传送到相同的分区号。 换句话说，输入数据的键空间必须以相同的方式分布在分区上。 这意味着，例如，使用Kafka的Java Producer API的应用程序必须使用相同的分区（比如生产者设置“partitioner.class”，也就是ProducerConfig.PARTITIONER_CLASS_CONFIG），使用Kafka的Streams API的应用程序必须使用相同的StreamPartitioner 诸如KStream＃to（）之类的操作。 好消息是，如果您碰巧在所有应用程序中使用默认的与分区相关的设置，则不必担心分区策略。   

为什么需要数据共分区？ 由于KStream-KStream，KTable-KTable和KStream-KTable联接是基于记录的键（如leftRecord.key == rightRecord.key）进行的，因此要求连接的输入流/ 按键分区。  
唯一的例外是KStream-GlobalKTable连接。 在这里，不需要共同分区，因为GlobalKTable底层变更日志流的所有分区都可用于每个KafkaStreams实例，即每个实例都具有变更日志流的完整副本。 此外，KeyValueMapper允许从KStream到GlobalKTable的非基于键的连接。

注意  
Kafka流部分验证了共同分区的要求：在分区分配步骤中，即在运行时，Kafka Streams验证连接两边的分区数是否相同。 如果不是，则会抛出TopologyBuilderException（运行时异常）。 请注意，Kafka Streams无法验证分割策略是否在连接的输入流/表之间匹配 - 由用户确定是否属于这种情况。

确保数据共分区：如果连接的输入尚未共同分区，则必须手动确保。 您可以按照下面列出的程序进行操作。
1. 识别连接中的输入KStream / KTable，Kafka底层主题的分区数量较少，我们称这个流/表为“SMALLER”，而连接的另一边则为“LARGER”。要了解卡夫卡主题的分区数量，可以使用CLI工具`bin/kafka-topics`和`--describe`选项。
2. 为“SMALLER”预先创建一个与“LARGER”分区数相同的新Kafka主题。我们把这个新的主题称为“重新分区的主题”。通常情况下，您可以使用CLI工具`bin/kafka-topics`和`--create`选项。
3. 在您的应用程序中，将“SMALLER”的数据重新写入新的Kafka主题。您必须确保在使用或通过写入数据时使用与“LARGER”相同的分区程序。
 - 如果“SMALLER”是`KStream：KStream＃to（“repartitioned-topic-for-smaller”）`。
 - 如果“SMALLER”是KTable：`KTable＃to（“repartitioned-topic-for-smaller”）`。
4. 在您的应用程序中，重新读取“重新分区的主题为更小”中的数据到一个新的KStream / KTable。
 - 如果“SMALLER”是KStream：`StreamsBuilder#stream("repartitioned-topic-for-smaller")`.
 - 如果“SMALLER”是KTable： `StreamsBuilder#table("repartitioned-topic-for-smaller")`.
5. 在您的应用程序中，执行“LARGER”和新的流/表之间的连接。

##### KStream-KStream 连接
KStream-KStream连接总是窗口连接，因为否则用于执行连接的内部状态存储区的大小（例如，滑动窗口或“缓冲区”）将无限增长。 对于流 - 流连接，重要的是要强调一边的新输入记录会为另一边的每个匹配记录产生连接输出，并且在给定的连接窗口中可以有多个这样的匹配记录（参见 例如下面的连接语义表中的时间戳15）。

连接输出记录有效地创建如下，利用用户提供的ValueJoiner：
```
KeyValue<K, LV> leftRecord = ...;
KeyValue<K, RV> rightRecord = ...;
ValueJoiner<LV, RV, JV> joiner = ...;

KeyValue<K, JV> joinOutputRecord = KeyValue.pair(
    leftRecord.key, /* by definition, leftRecord.key == rightRecord.key */
    joiner.apply(leftRecord.value, rightRecord.value)
  );
```

| 转换    | 描述    |
| :------------- | :------------- |
| Inner Join (windowed)<br><br>(KStream, KStream) → KStream      | 与另一个流执行此流的INNER JOIN。 即使这个操作是窗口化的，联合流将是KStream <K，...>类型，而不是KStream <Windowed <K>，...>。 （细节）<br><br>数据必须共同分区：双方的输入数据必须共同分区。<br><br>当且仅当流被标记为重新分区时（如果两者都被标记，两者都被重新分区），才会导致数据重新分区。<br><br>连接的几个变体存在，请参阅Javadocs的细节。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>连接是基于键的，即，连接谓词leftRecord.key == rightRecord.key和基于窗口的连接，即当且仅当它们的时间戳彼此“接近”时由用户定义，即两个输入记录被连接 - 提供JoinWindows，即该窗口在记录时间戳上定义了一个额外的连接谓词。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。 当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br>具有null键或null值的输入记录将被忽略，不会触发连接。<br><br>有关详细说明，请参阅本节底部的语义概述。      |
|Left Join (windowed)<br><br>(KStream, KStream) → KStream|对另一个流执行此流的LEFT JOIN。 即使这个操作是窗口化的，连接流将是KStream <K，...>类型，而不是KStream <Windowed <K>，...>。 （细节）<br><br>数据必须共同分区：双方的输入数据必须共同分区。<br><br>当且仅当流被标记为重新分区时（如果两者都被标记，两者都被重新分区），才会导致数据重新分区。<br><br>存在几个左连接的变体，详见Javadocs。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>连接是基于键的，即，连接谓词leftRecord.key == rightRecord.key和基于窗口的连接，即当且仅当它们的时间戳彼此“接近”时由用户定义，即两个输入记录被连接 - 提供JoinWindows，即该窗口在记录时间戳上定义了一个额外的连接谓词。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。 当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br>具有null键或null值的输入记录将被忽略，不会触发连接。<br><br>对于左侧没有任何匹配的每个输入记录，ValueJoiner将被调用`ValueJoiner＃apply（leftRecord.value，null）`; 这解释了下表中的timestamp = 3的行，它在LEFT JOIN列中列出[A，null]。<br><br>有关详细说明，请参阅本节底部的语义概述。|
|Outer Join (windowed)<br><br>(KStream, KStream) → KStream|用另一个流执行此流的OUTER JOIN。 即使这个操作是窗口化的，联合流将是KStream <K，...>类型，而不是KStream <Windowed <K>，...>。 （细节）<br><br>数据必须共同分区：双方的输入数据必须共同分区。<br><br>当且仅当流被标记为重新分区时（如果两者都被标记，两者都被重新分区），才会导致数据重新分区。<br><br>存在几个外连接的变体，详见Javadocs。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>连接是基于键的，即，连接断言leftRecord.key == rightRecord.key和基于窗口的连接，即当且仅当它们的时间戳彼此“接近”时由用户定义的两个输入记录才被连接 - 提供JoinWindows，即该窗口在记录时间戳上定义了一个额外的连接断言。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br>具有null键或null值的输入记录将被忽略，不会触发连接。<br><br>对于每一个没有任何匹配的输入记录，ValueJoiner将被分别调用`ValueJoiner＃apply（leftRecord.value，null）`或`ValueJoiner＃apply（null，rightRecord.value）`;这解释了下表中的timestamp = 3的行，它在OUTER JOIN列中列出了[A，null]（不同于LEFT JOIN，[null，x]也是可能的，但是表中没有这样的例子） 。<br><br>有关详细说明，请参阅本节底部的语义概述。|

stream-stream 连接的语义：下面将解释各种流 - 流连接变体的语义。 为了提高表的可读性，假设（1）所有记录具有相同的键（因此表中的键被省略），（2）所有记录属于单个连接窗口，以及（3）所有记录是 按时间戳顺序处理。 INNER JOIN，LEFT JOIN和OUTER JOIN列表示在连接的任一侧收到新的输入记录时，分别为join，leftJoin和outerJoin方法分别传递给用户提供的ValueJoiner的参数。 空的表格单元表示ValueJoiner根本不被调用。

|Timestamp	|Left (KStream)	|Right (KStream)	|(INNER) JOIN	|LEFT JOIN	|OUTER JOIN|
| :------------- | :------------- |:------------- |:------------- |:------------- |:------------- |
|1	|null	 	 	 	 |||||
|2	|null	 	 	 |||||
|3	|A	 	 ||	|[A, null]	|[A, null]
|4	| 	|a	|[A, a]	|[A, a]	|[A, a]
|5	|B	 	||[B, a]|	[B, a]	|[B, a]
|6	|| 	b	|[A, b], [B, b]|	[A, b], [B, b]|	[A, b], [B, b]|
|7	|null|||||	 	 	 	 
|8	 ||	null	 	 	 ||||
|9	|C||	 	[C, a], [C, b]|	[C, a], [C, b]|	[C, a], [C, b]|
|10	|| 	c|	[A, c], [B, c], [C, c]	|[A, c], [B, c], [C, c]	[A, c], [B, |c], [C, c]|
|11	 ||	null	 	 	 ||||
|12	|null	 	 	 	 |||||
|13	|| 	null	 	 	 ||||
|14	|| 	d|	[A, d], [B, d], [C, d]|[A, d], [B, d], [C, d]|[A, d], [B, d], [C, d]
|15	|D||	 	[D, a], [D, b], [D, c], [D, d]|[D, a], [D, b], [D, c], [D, d]|[D, a], [D, b], [D, c], [D, d]

##### KTable-KTable连接
KTable-KTable连接始终是非窗口连接。 它们旨在与关系数据库中的对应方保持一致。 两个KTables的更改日志流都被物化到本地状态存储库中，以表示其双重表的最新快照。 连接结果是一个新的KTable，表示连接操作的更改日志流。

连接输出记录有效地创建如下，利用用户提供的ValueJoiner：
```
KeyValue<K, LV> leftRecord = ...;
KeyValue<K, RV> rightRecord = ...;
ValueJoiner<LV, RV, JV> joiner = ...;

KeyValue<K, JV> joinOutputRecord = KeyValue.pair(
    leftRecord.key, /* by definition, leftRecord.key == rightRecord.key */
    joiner.apply(leftRecord.value, rightRecord.value)
  );
```
| 转换     |描述     |
| :------------- | :------------- |
| Inner Join <br><br> (KTable, KTable) → KTable       | 执行此表与另一个表的INNER JOIN。 结果是一个不断更新的KTable表示连接的“当前”结果。 （细节）<br><br>数据必须共同分区：双方的输入数据必须共同分区。详细的行为：<br><br>连接是基于关键字的，即连接谓词leftRecord.key == rightRecord.key。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。 当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br>带有空键的输入记录被忽略，不会触发连接。<br><br>具有空值的输入记录被解释为相应键的墓碑，其指示从表中删除键。 墓碑不会触发连接。 当接收到输入逻辑删除时，如果需要的话（即仅当相应的键实际上已经存在于加入结果KTable中），才将输出逻辑删除直接转发给加入结果KTable。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>有关详细说明，请参阅本节底部的语义概述。|
|Left Join<br><br>(KTable, KTable) → KTable|与另一个表执行此表的LEFT JOIN。 （细节）<br><br>数据必须共同分区：双方的输入数据必须共同分区。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>连接是基于关键字的，即连接谓词为leftRecord.key == rightRecord.key。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br> - 带有空键的输入记录被忽略，不会触发连接。<br><br> - 具有空值的输入记录被解释为相应键的墓碑，其指示从表中删除键。墓碑不会触发连接。当接收到输入逻辑删除时，如果需要（即只有当相应的键实际上已经存在于连接结果KTable中），才将输出逻辑删除直接转发给连接结果KTable。<br><br>对于左侧没有任何匹配的每个输入记录，ValueJoiner将被调用ValueJoiner＃apply（leftRecord.value，null）;这解释了下表中的timestamp = 3的行，它在LEFT JOIN列中列出[A，null]。<br><br>有关详细说明，请参阅本节底部的语义概述。|
|Outer Join<br><br>(KTable, KTable) → KTable|与另一个表执行此表的OUTER JOIN。 （细节）<br><br>数据必须共同分区：双方的输入数据必须共同分区。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>连接是基于关键字的，即连接谓词为leftRecord.key == rightRecord.key。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br>带有空键的输入记录被忽略，不会触发连接。<br><br>具有空值的输入记录被解释为相应键的墓碑，其指示从表中删除键。墓碑不会触发连接。当接收到输入逻辑删除时，如果需要（即只有当相应的键实际上已经存在于连接结果KTable中），才将输出逻辑删除直接转发给连接结果KTable。<br><br>对于每一个没有任何匹配的输入记录，ValueJoiner将被分别调用ValueJoiner＃apply（leftRecord.value，null）或ValueJoiner＃apply（null，rightRecord.value）;这解释了下表中的timestamp = 3和timestamp = 7的行，分别在OUTER JOIN列中列出[A，null]和[null，b]。<br><br>有关详细说明，请参阅本节底部的语义概述。|

表连接的语义：下面解释各种表格表格连接变体的语义。 为了提高表的可读性，可以假设（1）所有记录具有相同的键（因而表中的键被省略），并且（2）所有记录按时间戳顺序处理。 INNER JOIN，LEFT JOIN和OUTER JOIN列表示在连接的任一侧收到新的输入记录时，分别为join，leftJoin和outerJoin方法分别传递给用户提供的ValueJoiner的参数。 一个空的表格单元表示ValueJoiner根本不被调用。

|Timestamp|	Left (KTable)|	Right (KTable)|(INNER) JOIN	|LEFT JOIN	|OUTER JOIN|
| :------------- | :------------- | :------------- | :------------- | :------------- | :------------- |
|1	|null (tombstone)	|||| 	 	 	 |
|2	| 	|null (tombstone)|||	 	 	 |
|3	|A|||	 	 	[A, null]	|[A, null]|
|4	| 	|a|	[A, a]|	[A, a]|	[A, a]|
|5	|B	|| 	[B, a]|	[B, a]	|[B, a]|
|6	| 	|b|	[B, b]|	[B, b]	|[B, b]|
|7	|null (tombstone)|	| 	null (tombstone)|	null (tombstone)|	[null, b]|
|8	| 	|null (tombstone)|||	 	 	null (tombstone)|
|9	|C	||| 	 	[C, null]	|[C, null]|
|10|	 	|c|	[C, c]	|[C, c]|	[C, c]|
|11|	 	|null (tombstone)|	null| (tombstone)	|[C, null]|	[C, null]|
|12|	null (tombstone)|	 	 	||null (tombstone)|	null (tombstone)|
|13|	 	|null (tombstone)|||	 	 	 |
|14|	 	|d	||| 	 	[null, d]|
|15|	D	|| 	[D, d]	|[D, d]|	[D, d]|

##### KStream-KTable 连接
KStream-KTable连接始终是非窗口连接。 它们允许您在接收到来自KStream（记录流）的新记录时，对KTable（更新日志流）执行表查找。 一个示例用例是使用最新的用户配置文件信息（KTable）来丰富用户活动（KStream）。

连接输出记录有效地创建如下，利用用户提供的ValueJoiner：
```
KeyValue<K, LV> leftRecord = ...;
KeyValue<K, RV> rightRecord = ...;
ValueJoiner<LV, RV, JV> joiner = ...;

KeyValue<K, JV> joinOutputRecord = KeyValue.pair(
    leftRecord.key, /* by definition, leftRecord.key == rightRecord.key */
    joiner.apply(leftRecord.value, rightRecord.value)
  );
```
| 转换 | 描述 |
| :------------- | :------------- |
| Inner Join <br><br>(KStream, KTable) → KStream | 执行一个INNER JOIN这个流与表，有效地做一个表查找。 （细节）<br><br>数据必须共同分区：双方的输入数据必须共同分区。<br><br>导致数据流重新分区，当且仅当流被标记为重新分区。<br><br>连接的几个变体存在，请参阅Javadocs的细节。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>连接是基于关键字的，即连接谓词leftRecord.key == rightRecord.key。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。 当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br>只有左侧输入记录（流）触发连接。 右侧（表格）的输入记录仅更新内部右侧连接状态。<br><br>具有null键或null值的流的输入记录被忽略，不会触发连接。<br><br>具有空值的表的输入记录被解释为相应键的逻辑删除，这表示从表中删除键。 墓碑不会触发连接。<br><br>有关详细说明，请参阅本节底部的语义概述。 |
| Left Join<br><br>(KStream, KTable) → KStream      | 对该表执行此流的LEFT JOIN，从而有效地进行表查找。 （细节）<br><br>数据必须共同分区：双方的输入数据必须共同分区。<br><br>导致数据流重新分区，当且仅当流被标记为重新分区。<br><br>存在几个左连接的变体，详见Javadocs。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br> 详细的行为：<br><br> 连接是基于关键字的，即连接谓词为leftRecord.key == rightRecord.key。<br><br> 每当收到新的输入时，连接将在下面列出的条件下被触发。当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br> 只有左侧输入记录（流）触发连接。右侧（表格）的输入记录仅更新内部右侧连接状态。<br><br> 具有null键或null值的流的输入记录将被忽略，不会触发连接。<br><br> 具有空值的表的输入记录被解释为相应键的逻辑删除，这表示从表中删除键。墓碑不会触发连接。<br><br> 对于左侧没有任何匹配的每个输入记录，ValueJoiner将被调用ValueJoiner＃apply（leftRecord.value，null）;这解释了下表中的timestamp = 3的行，它在LEFT JOIN列中列出[A，null]。<br><br> 有关详细说明，请参阅本节底部的语义概述。|

连接的语义：下面将解释各种流表连接变体的语义。 为了提高表的可读性，我们假设（1）所有记录具有相同的密钥（因此我们省略了表中的密钥），并且（2）所有记录按照时间戳顺序处理。 INNER JOIN和LEFT JOIN列分别表示在连接的任何一边收到新的输入记录时，分别为join和leftJoin方法分别传递给用户提供的ValueJoiner的参数。 一个空的表格单元表示ValueJoiner根本不被调用。

|Timestamp	|Left (KStream)|	Right (KTable)|	(INNER) JOIN	|LEFT JOIN|
| :------------- | :------------- | :------------- | :------------- | :------------- |
|1	null|	 	 	 ||||
|2	| 	|null (tombstone)	 	 |||
|3	|A	| 	 ||[A, null]|
|4	| 	|a	 |	 ||
|5	|B	| 	|[B, a]|	[B, a]
|6	| 	|b	 	 |||
|7	|null	 	 	 ||||
|8	| 	|null (tombstone)	 	 |||
|9	|C	 |||	 	[C, null]
|10|	 	|c|||	 	 
|11|	 	|null	 	 |||
|12|	null||||	 	 	 
|13|	 	|null	 	 |||
|14|	 	|d	 	 |||
|15|	D	|| 	[D, d]	|[D, d]

##### KStream-GlobalKTable 连接
KStream-GlobalKTable连接始终是非窗口连接。它们允许您在接收到来自KStream（记录流）的新记录时，对GlobalKTable（整个更新日志流）执行表查找。一个示例用例可以是“star queries”或“star join”，您可以在其中使用最新的用户配置文件信息（GlobalKTable）和更多的上下文信息（更多的GlobalKTables）丰富用户活动流（KStream）。

在高层次上，KStream-GlobalKTable连接与KStream-KTable连接非常相似。然而，与分区表相比，全局表为您提供了更多的灵活性，

- 它们不需要数据共分区。
- 他们允许有效的“明星加盟”;即将大规模的“事实”流与“维度”表结合起来
- 他们允许加入反对外键;即可以在表中查找数据，而不仅仅是通过流中记录的键，还可以通过记录值中的数据来查找。
- 他们使得许多用例可行，您必须处理严重倾斜的数据，因此受到热分区的困扰。
- 当您需要连续执行多个连接时，它们通常比分区的KTable对象更高效。

连接输出记录有效地创建如下，利用用户提供的ValueJoiner：
```
KeyValue<K, LV> leftRecord = ...;
KeyValue<K, RV> rightRecord = ...;
ValueJoiner<LV, RV, JV> joiner = ...;

KeyValue<K, JV> joinOutputRecord = KeyValue.pair(
    leftRecord.key, /* by definition, leftRecord.key == rightRecord.key */
    joiner.apply(leftRecord.value, rightRecord.value)
  );
```

| 转换 | 描述 |
| :------------- | :------------- |
| Inner Join<br><br>(KStream, GlobalKTable) → KStream      | 执行此流与全局表的INNER JOIN，从而有效地进行表查找。 （细节）<br><br>GlobalKTable在KafkaStreams实例的（重新）启动时被完全引导，这意味着该表完全填充了启动时可用的底层主题中的所有数据。 实际的数据处理只在引导完成后才开始。<br><br>导致数据流重新分区，当且仅当流被标记为重新分区。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>连接是间接基于关键的，即用连接谓词KeyValueMapper＃apply（leftRecord.key，leftRecord.value）== rightRecord.key。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。 当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br>只有左侧输入记录（流）触发连接。 右侧（表格）的输入记录仅更新内部右侧连接状态。<br><br>具有null键或null值的流的输入记录被忽略，不会触发连接。<br><br>用空值输入记录的表被解释为墓碑，表示从表中删除记录键。 墓碑不会触发连接。     |
| Left Join<br><br>(KStream, GlobalKTable) → KStream     | 对全局表执行此流的LEFT JOIN，有效地进行表查找。 （细节）<br><br>GlobalKTable在KafkaStreams实例的（重新）启动时被完全引导，这意味着该表完全填充了启动时可用的底层主题中的所有数据。 实际的数据处理只在引导完成后才开始。<br><br>导致数据流重新分区，当且仅当流被标记为重新分区。<br><br>[代码参考](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html)<br><br>详细的行为：<br><br>连接是间接基于关键的，即用连接谓词`KeyValueMapper＃apply（leftRecord.key，leftRecord.value）== rightRecord.key`。<br><br>每当收到新的输入时，连接将在下面列出的条件下被触发。 当它被触发时，用户提供的ValueJoiner将被调用来产生连接输出记录。<br><br>只有左侧输入记录（流）触发连接。 右侧（表格）的输入记录仅更新内部右侧连接状态。<br><br>具有null键或null值的流的输入记录被忽略，不会触发连接。<br><br>用空值输入记录的表被解释为墓碑，表示从表中删除记录键。 墓碑不会触发连接。<br><br>对于左侧没有任何匹配的每个输入记录，ValueJoiner将被调用`ValueJoiner＃apply（leftRecord.value，null）`。    |

流表连接的语义：连接语义与KStream-KTable连接相同。 唯一的区别是，对于KStream-GlobalKTable连接，在表查找之前，左输入记录首先用用户提供的KeyValueMapper“映射”到表的键空间中。

#### <h4 id="窗口化">窗口化</h4>

#### <h4 id="使用有状态集成处理器API的自定义处理器和转换器">使用有状态集成处理器API的自定义处理器和转换器</h4>
