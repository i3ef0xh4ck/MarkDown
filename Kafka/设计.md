# Kafka 设计
[原文地址](https://kafka.apache.org/documentation/#design)

## 动机
我们设计的Kafka是为了能够充当一个统一的平台来处理大公司可能拥有的所有实时数据馈送。要做到这一点，我们必须考虑相当广泛的用例。

它必须具有高吞吐量才能支持高容量事件流，例如实时日志聚合。

它需要正常处理大量数据积压，以便能够支持来自离线系统的定期数据加载。

这也意味着系统必须处理低延迟传输以处理更传统的消息传递用例。

我们希望支持对这些提要进行分区，分布式，实时处理以创建新的派生提要。这激发了我们的分区和消费者模式。

最后，在流被送入其他数据系统进行服务的情况下，我们知道系统必须能够在出现机器故障时保证容错。

支持这些用途导致我们设计了许多独特的元素，更类似于数据库日志而不是传统的消息传递系统。我们将在下面的章节中概述设计的一些要素。

# 持久化
不要害怕文件系统！

Kafka在很大程度上依赖文件系统来存储和缓存消息。人们普遍认为“磁盘速度缓慢”让人们怀疑一个持久结构可以提供有竞争力的性能。实际上，磁盘比人们预期的要慢得多，速度也要快得多，这取决于它们的使用方式。而且设计合理的磁盘结构通常可以和网络一样快。

关于磁盘性能的关键事实是硬盘的吞吐量与过去十年磁盘寻找的延迟不同。因此，使用6个7200rpm SATA RAID-5阵列的JBOD配置上的线性写入性能约为600MB /秒，但随机写入的性能仅约为100k / sec，相差6000X以上。这些线性读取和写入是所有使用模式中最可预测的，并且在操作系统上进行了大量优化。现代操作系统提供预读和后写技术，以大块数倍预取数据，并将较小的逻辑写入分组到大型物理写入中。有关此问题的进一步讨论，请参阅此ACM队列文章;他们实际上发现顺序磁盘访问在某些情况下可能比随机访问存储更快！

为弥补这种性能差异，现代操作系统在使用主存储器进行磁盘高速缓存时变得越来越积极。当内存被回收时，现代操作系统会高兴地将所有空闲内存转移到磁盘缓存，而性能损失很小。所有的磁盘读写都将通过这个统一的缓存。如果不使用直接I / O，则无法轻松关闭此功能，因此即使进程维护数据的进程内缓存，该数据也可能会在操作系统页面缓存中复制，从而有效地存储所有内容两次。

此外，我们正在构建JVM之上，任何花时间使用Java内存的人都知道两件事：

1. 对象的内存开销很高，通常会使所存储数据的大小加倍（或更糟糕）。

2. 随着堆内数据的增加，Java垃圾收集变得越来越复杂和缓慢。

由于使用文件系统和依赖页面缓存的这些因素优于维护内存缓存或其他结构，因此我们至少将可用缓存翻了一番，方法是自动访问所有可用内存，并且可能再次通过存储压缩包字节结构而不是单个对象。这样做会导致在32GB机器上高达28-30GB的缓存，而不受GC惩罚。此外，即使重新启动服务，该缓存也会保持温暖，而进程内缓存需要在内存中重新构建（对于10GB缓存可能需要10分钟），否则将需要以完全冷的缓存开始（这可能意味着糟糕的初始表现）。这也极大地简化了代码，因为所有用于维持高速缓存和文件系统之间一致性的逻辑现在都在OS中，这比一次性的进程内尝试更有效且更正确。如果您的磁盘使用情况支持线性读取，则预读使用每个磁盘读取的有用数据有效地预先填充此缓存。

这表明一个非常简单的设计：当我们在空间不足的时候，我们不会尽可能地保持内存中的内容，并且在文件系统中将其全部冲洗到一个恐慌状态，我们将其反转。所有数据都立即写入文件系统的永久日志，而不必冲刷到磁盘。实际上，这仅仅意味着它被转移到内核的页面缓存中。

这种以页面缓存为中心的设计风格在一篇关于Varnish设计的[文章](http://varnish-cache.org/wiki/ArchitectNotes)中描述（伴随着健康的傲慢）。

##  效率
我们已经为效率付出了很大的努力。我们的主要用例之一是处理网络活动数据，这是非常高的数据量：每个页面视图可能会产生数十个写入。此外，我们假设发布的每条消息都被至少一个消费者（通常是很多消费者）阅读，因此我们努力使消费尽可能便宜。

我们还发现，从建立和运行一系列类似系统的经验来看，效率是实现多租户有效运营的关键。如果下游基础设施服务由于应用程序使用量小而容易成为瓶颈，那么这些小的更改往往会产生问题。通过非常快的速度，我们可以帮助确保应用程序在基础架构之前在负载下翻车。当尝试运行支持集中式群集上数十或数百个应用程序的集中式服务时，这一点尤其重要，因为使用模式的变化几乎每天都在发生。

我们在前一节讨论了磁盘效率。一旦消除了较差的磁盘访问模式，在这种类型的系统中存在两种低效率的常见原因：太多的小I / O操作和过多的字节复制。

小的I / O问题发生在客户端和服务器之间以及服务器自己的持久操作中。

为了避免这种情况，我们的协议建立在一个“消息集”抽象的基础上，自然将消息分组在一起。这允许网络请求将消息分组在一起，并分摊网络往返的开销，而不是一次发送单个消息。服务器依次将消息块附加到其日志中，并且消费者一次获取大型线性块。

这个简单的优化产生了数量级的加速。批处理导致更大的网络数据包，更大的顺序磁盘操作，连续的内存块等等，所有这些都允许Kafka将随机消息写入的突发流转换为流向消费者的线性写入。

另一个低效率是在字节复制中。在低消息速率下，这不是问题，但是在负载情况下，影响是显着的。为了避免这种情况，我们使用由生产者，代理和消费者共享的标准二进制消息格式（因此数据块可以在它们之间进行无需修改的传输）。

由代理维护的消息日志本身就是一个文件目录，每个文件都由一系列消息集合填充，这些消息集合以生产者和消费者使用的相同格式写入磁盘。 保持这种通用格式可以优化最重要的操作：持久日志块的网络传输。 现代unix操作系统提供高度优化的代码路径，用于将数据从页面缓存传输到套接字; 在Linux中，这是通过sendfile系统调用完成的。

要理解sendfile的影响，了解数据从文件传输到套接字的公共数据路径非常重要：  
1. 操作系统从磁盘读取数据到内核空间的pagecache中  
2. 应用程序从内核空间读取数据到用户空间缓冲区中  
3. 应用程序将数据写回内核空间到套接字缓冲区  
4. 操作系统将数据从套接字缓冲区复制到通过网络发送的NIC缓冲区  

这显然是低效的，有四个副本和两个系统调用。 使用sendfile，可以通过允许操作系统直接将数据从页缓存（pagecache）发送到网络来避免重新复制。 所以在这个优化的路径中，只需要最终拷贝到NIC缓冲区。

我们期望一个常见的用例在一个主题上成为多个消费者。 使用上面的零拷贝优化，数据被复制到页缓存（pagecache）中一次，并在每次使用时重用，而不是存储在内存中，并且每次读取时拷贝到用户空间。 这允许消息以接近网络连接限制的速率消耗。

这种页面缓存和发送文件的结合意味着，在消费者大多被捕获的卡夫卡群集中，您将看不到磁盘上的读取活动，因为它们将完全从缓存中提供数据。

有关sendfile和Java中零拷贝支持的更多背景信息，请参阅[本文](http://www.ibm.com/developerworks/linux/library/j-zerocopy)。
### 端到端批量压缩
在某些情况下，瓶颈实际上不是CPU或磁盘，而是网络带宽。 对于需要通过广域网在数据中心之间发送消息的数据管道，情况尤其如此。 当然，用户可以一次压缩一条消息，而不需要kafka提供任何支持，但是这会导致压缩比非常低，因为冗余的多少是由于相同类型消息之间的重复（例如，字段名称 JSON或Web日志中的用户代理或公共字符串值）。 有效的压缩需要将多个消息压缩在一起，而不是单独压缩每个消息。

kafka以高效的批处理格式支持这一点。 一批消息可以压缩在一起并以这种形式发送到服务器。 这批消息将以压缩格式写入，并将保持压缩在日志中，并仅由消费者解压缩。

Kafka支持GZIP，Snappy和LZ4压缩协议。 更多关于压缩的细节可以在[这里](https://cwiki.apache.org/confluence/display/KAFKA/Compression)找到。

## 生产者
### 负载均衡
生产者将数据直接发送给作为分区领导者的代理，而不需要任何中间路由层。为了帮助生产者做到这一点，所有Kafka节点都可以回答关于哪些服务器处于活动状态的元数据请求，以及主题分区的领导者在任何给定时间的位置，以便生产者适当地指导其请求。

客户端控制它将消息发布到哪个分区。这可以随机完成，实现一种随机负载平衡，或者可以通过某种语义分区功能完成。我们公开了用于语义分区的接口，方法是允许用户指定要分区的密钥并使用它来散列分区（如果需要，还可以选择覆盖分区函数）。例如，如果选择的密钥是用户ID，则给定用户的所有数据都将被发送到同一分区。这反过来将允许消费者对他们的消费做出当地假设。这种分区风格明确地设计为允许消费者对区域敏感的处理。
### 异步发送
批次是效率的重要推动因素之一，为了实现批量生产，Kafka生产商将尝试在内存中积累数据，并在单个请求中发送更大批次的数据。 批处理可以被配置为累加不超过固定数量的消息，并且不超过某个固定的延迟限制（比如64k或10ms）。 这允许发送更多字节的累积，并且在服务器上几个较大的I / O操作。 这种缓冲是可配置的，并提供了一种机制来折中少量额外的延迟以获得更好的吞吐量。

有关[配置](https://kafka.apache.org/documentation/#producerconfigs)的详细信息和生产者的[api](http://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html)可以在文档的其他地方找到。

## 消费者
Kafka消费者通过向希望消费的分区的经纪人发出“获取”请求而工作。 消费者在每个请求的日志中指定其偏移量，并从该位置接收一段日志。 因此，消费者对这个位置具有重要的控制权，并且可以倒带它以在需要时重新消费数据。
### Push vs. pull
我们考虑的最初问题是消费者是否应该从经纪商或经纪商提取数据，以便将数据推送给消费者。在这方面，Kafka采用更传统的设计，由大多数信息系统共享，数据从胜场这推送给中间件，并由消费者从中间件处提取。一些以日志为中心的系统（如Scribe和Apache Flume）采用了一种非常不同的基于推送的路径，将数据推送到下游。这两种方法都有优点和缺点。然而，基于推送的系统难以处理不同的消费者，因为中间件控制着数据传输的速度。目标通常是消费者能够以最大可能的速度消费;不幸的是，在推送系统中，这意味着消费者的消费率低于生产率时（本质上是拒绝服务攻击），消费者往往会不知所措。基于拉取的系统具有更好的属性，消费者只需落后并在可能的时候赶上。这可以通过某种退让协议来减轻，消费者可以通过这种退让协议来表明它已经不堪重负，但让消费者充分利用（但从未过度利用）消费者的转化率似乎比看起来更复杂。先前以这种方式构建系统的尝试使我们采用更传统的拉式模型。

基于拉取的系统的另一个优点是它可以将发送给消费者的数据进行大量批量处理。基于推送的系统必须选择立即发送请求或累积更多数据，然后在不知道下游消费者是否能够立即处理它的情况下发送。如果调整为低延迟，则这将导致一次只发送一条消息，以便传输最终被缓冲，这是浪费的。基于拉取的设计修复了这种情况，因为消费者总是将所有可用消息拖到日志中的当前位置（或达到某个可配置的最大大小）之后。所以人们可以获得最佳的批次而不会引入不必要的延迟

一个天真的基于拉取的系统的缺陷是，如果中间件没有数据，消费者可能会以紧张的循环结束投票，实际上是忙于等待数据到达。为了避免这种情况，我们在我们的pull请求中有参数，它允许消费者请求以“长轮询”方式阻塞，等待数据到达（并且可选地等待，直到给定数量的字节可用以确保大的传输大小）。

你可以想象其他可能的设计，这只会拉，端到端。胜场这会在当地写信给当地的日志，而中间件会从消费者那里拉走。经常提出一种类似的“存储和转发”生产者。这很有趣，但我们觉得不太适合我们有数千个生产者的目标用例。我们大规模运行持久数据系统的经验使我们感到，跨越许多应用程序在系统中涉及数以千计的磁盘实际上不会使事情变得更加可靠，并且会成为操作的噩梦。实际上，我们发现我们可以在不需要生产者持续性的情况下大规模地运行带有强大SLA的管道。

### 消费者位置

令人惊讶的是，跟踪已消费的是消息传递系统的关键性能点之一。

大多数消息传递系统都保留关于代理上消费的消息的元数据。也就是说，当消息发送给消费者时，中间件要么立即在本地记录该事实，要么等待消费者的确认。这是一个相当直观的选择，实际上对于单台机器服务器来说，并不清楚这个状态可能发生在哪里。由于许多消息传递系统中用于存储的数据结构规模较小，因此这也是一个实用的选择 - 因为代理知道消耗的是什么，它可以立即删除它，保持数据量小。

可能不明显的是，让中间件和消费者就已经消费的东西达成一致并不是一个小问题。如果代理将消息记录为在每次通过网络发送时立即使用，那么如果消费者未能处理该消息（比如说因为崩溃或请求超时等等），则该消息将丢失。为了解决这个问题，许多消息传递系统增加了一个确认功能，这意味着消息只在被发送时标记为发送而不被消费;代理等待消费者的特定确认将消息记录为已消费。这个策略解决了丢失信息的问题，但是却产生了新的问题。首先，如果消费者在发送确认之前处理消息但失败，则该消息将被消费两次。第二个问题是性能问题，现在代理必须保持每个消息的多个状态（首先锁定它，以便它不会再次发出，然后将其标记为永久消耗以便将其删除）。棘手的问题必须得到处理，比如如何处理发送但未被确认的消息。

Kafka处理这个不同。我们的主题被分成一组完全有序的分区，每个分区在任何给定的时间都由每个订阅消费者组中的一名消费者使用。这意味着消费者在每个分区中的位置只是一个整数，即要消费的下一个消息的偏移量。这使得状态变得非常小，每个分区只有一个数字。这个状态可以定期检查点。这使消息确认的等价物非常便宜。

这个决定有一个好处。消费者可以故意退回到旧的偏移量并重新使用数据。这违反了队列的共同合同，但是对于许多消费者来说这是一个重要特征。例如，如果消费者代码有一个错误，并且在一些消息被消费后被发现，那么消费者可以在错误修复后重新使用这些消息。

### 离线数据加载
可伸缩持久性允许消费者仅定期消费的可能性，例如批量数据加载，定期将数据批量加载到离线系统（如Hadoop或关系数据仓库）中。

在Hadoop的情况下，我们通过在单个映射任务上分割负载来并行化数据负载，每个节点/主题/分区组合都有一个负载，从而在负载中实现完全并行化。 Hadoop提供任务管理，失败的任务可以重新启动而没有重复数据的危险 - 它们只需从原始位置重新启动即可。

## 消息传递语义
现在我们对生产者和消费者的工作方式有了一些了解，让我们来讨论Kafka在生产者和消费者之间提供的语义保证。 显然，可以提供多种可能的消息传递保证：
- 最多一次 - 消息可能会丢失，但永远不会重新发送。
- 至少一次 - 消息永远不会丢失，但可以重新发送。
- 仅一次，这是人们真正想要的，每条消息只传递一次。

值得注意的是，这分成两个问题：发布消息的持久性保证以及消费消息时的保证。
许多系统声称提供“恰好一次”的交付语义，但重要的是要阅读细则，这些声明大多是误导性的（即它们不转化为消费者或生产者可能失败的情况，消费者进程或写入磁盘的数据可能丢失的情况）。

卡夫卡的语义是直截了当的。在发布消息时，我们将消息的概念“提交”到日志中。一旦发布的消息被提交，只要一个代理复制此消息的分区的代理仍然“活着”，它就不会丢失。承诺消息的定义，活动分区以及我们尝试处理哪些类型的故障的描述将在下一节中更详细地描述。现在让我们假设一个完美无损的经纪人，并试图理解生产者和消费者的保证。如果生产者试图发布消息并遇到网络错误，则无法确定在提交消息之前或之后是否发生此错误。这与使用自动生成的键插入到数据库表中的语义相似。

在0.11.0.0之前，如果生产者未能收到指示消息已提交的响应，则除了重新发送消息之外别无选择。这提供了至少一次传送语义，因为如果原始请求实际上已经成功，则可以在重新发送期间再次将消息写入日志。自0.11.0.0开始，Kafka生产者还支持一个幂等传送选项，它保证重新发送不会在日志中导致重复条目。为了达到这个目的，代理人为每个生产者分配一个ID并且使用由生产者发送的序列号和每个消息来消除重复消息。还从0.11.0.0开始，生产者支持使用类似事务的语义将消息发送到多个主题分区的能力：即，所有消息都被成功写入或者没有任何消息。这个主要用例恰好在卡夫卡话题之间进行一次处理（如下所述）。

并非所有用例都需要这种强有力的保证。对于对延迟敏感的用途，我们允许生产者指定其期望的耐久性级别。如果生产者指定它想要等待提交的消息，则这可能需要10毫秒的量级。然而，制作者也可以指定它想要完全异步执行发送，或者它只想等到领导者（但不一定是跟随者）才有消息。

现在让我们从消费者的角度来描述语义。所有副本都具有相同的日志和相同的偏移量。消费者控制其在此日志中的位置。如果消费者从未崩溃，它可以将这个位置存储在内存中，但是如果消费者失败了，并且我们希望这个主题分区被另一个进程接管，那么新进程将需要选择一个合适的开始处理的位置。假设消费者读取一些消息 - 它有几个选项用于处理消息并更新其位置。

1. 它可以读取消息，然后保存它在日志中的位置，并最终处理消息。在这种情况下，消费者进程可能会在保存其位置之后但在保存其消息处理输出之前崩溃。在这种情况下，接管处理的过程将从保存的位置开始，即使该位置之前的一些消息尚未处理。这对应于“最多一次”的语义，如在消费者失败的情况下可能不会处理消息。

2. 它可以读取消息，处理消息并最终保存其位置。在这种情况下，处理消息之后但在保存其位置之前，消费者进程可能会崩溃。在这种情况下，当新进程接管它接收到的前几条消息时已经处理完毕。这对应于消费者失败情况下的“至少一次”语义。在许多情况下，消息具有主键，因此更新是幂等的（接收相同的消息两次只是用另一个副本覆盖记录）。

那么，一次语义（即你真正想要的东西）呢？当从Kafka话题中消费并产生到另一个话题（如在Kafka Streams应用程序中）时，我们可以利用上面提到的0.11.0.0中的新事务生成器功能。消费者的位置作为消息存储在主题中，所以我们可以在与接收处理的数据的输出主题相同的事务中向Kafka写入偏移量。如果交易中止，则消费者的位置将恢复到其旧值，并且根据其“隔离级别”，输出主题上的生成数据对其他消费者而言将不可见。在默认的“read_uncommitted”隔离级别中，消费者即使是中止事务的一部分，所有消息都是可见的，但在“read_committed”中，消费者只会从已提交的事务中返回消息（以及任何不属于的交易）。

在写入外部系统时，限制在于需要将消费者的位置与实际存储为输出的位置进行协调。实现这一目标的经典方法是在消费者位置的存储与消费者输出的存储之间引入两阶段提交。但是，这可以通过让消费者将其偏移量存储在与其输出相同的位置来更简单地处理。这样更好，因为消费者可能希望写入的许多输出系统不支持两阶段提交。作为一个例子，考虑一个Kafka Connect连接器，该连接器在HDFS中填充数据以及读取数据的偏移量，以确保数据和偏移量都被更新，或者两者都不更新。对于需要这些更强大语义的许多其他数据系统，我们遵循类似模式，并且对于这些数据系统，消息没有允许重复数据删除的主键。

因此，有效地Kafka支持在Kafka Streams中准确传送一次，并且在传送和处理Kafka主题之间的数据时，事务性生产者/消费者通常可用于提供准确一次传送。对于其他目的地系统的准确一次交付通常需要与此类系统合作，但Kafka提供了实现这种可行的偏移量（另请参阅Kafka Connect）。否则，Kafka默认保证至少一次交付，并且允许用户在处理一批消息之前禁用生产者的重试和消费者的偏移，从而实现至多一次交付。

## 复制
Kafka通过可配置数量的服务器为每个主题的分区复制日志（您可以在逐个主题的基础上设置此复制因子）。这样，当群集中的服务器发生故障时，可以自动故障转移到这些副本，以便在出现故障时保持可用状态。

其他消息传递系统提供了一些与复制相关的功能，但是，在我们（完全有偏见的）看来，这似乎是一个不太常用的东西，并且有很大的缺点：从属处于非活动状态，吞吐量受到严重影响，繁琐的手动配置等.Kafka默认使用复制 - 事实上，我们将未复制的主题作为复制主题的复制主题实施。

复制单位是主题分区。在非失败条件下，Kafka中的每个分区都有一个单独的领导者和零个或多个关注者。包括领导者在内的副本总数构成复制因素。所有读取和写入都转到分区的领导。通常情况下，中间商比中间商多得多，而且领导者平均分配给经纪商。追随者的日志与领导者的日志相同 - 都以相同的顺序具有相同的偏移量和消息（尽管当然，在任何给定时间，领导者可能在其日志末尾有几条尚未复制的消息）。

追随者就像普通的Kafka消费者一样消费领导者的消息，并将他们应用到他们自己的日志中。让追随者从领导者身上取得好成绩，可以让追随者自然地将他们正在应用到他们的日志中的日志条目分组在一起。

与大多数分布式系统一样，自动处理故障需要精确定义节点“活跃”意味着什么。对于Kafka节点而言，活跃度有两个条件

1. 节点必须能够维护与ZooKeeper的会话（通过ZooKeeper的心跳机制）

2. 如果它是一个从节点，它必须复制发生在领导者身上的写作，而不会落后于“太远”

我们将满足这两个条件的节点称为“同步”，以避免“活着”或“失败”的模糊性。领导跟踪“同步”节点集。如果追随者死亡，被卡住或落后，领导将从同步副本列表中删除它。确定滞留和滞后副本由replica.lag.time.max.ms配置控制。
在分布式系统术语中，我们只尝试处理节点突然停止工作然后恢复（可能不知道它们已经死亡）的失败/恢复模式。卡夫卡不处理所谓的“拜占庭式”故障，其中节点会产生任意或恶意的响应（可能是由于错误或犯规行为）。

现在我们可以更准确地定义，当该分区的所有同步副本已将其应用于其日志时，就认为该消息已被提交。只有承诺的消息被分发给消费者。这意味着消费者不必担心如果领导失败，可能会看到可能丢失的消息。另一方面，生产者可以选择等待消息的提交与否，具体取决于他们在延迟与耐久性之间的权衡。此首选项由生产者使用的acks设置控制。请注意，主题有一个设置，用于在生产者请求确认消息已写入完整的同步副本集时检查的“最小数量”的同步副本。如果生产者请求较不严格的确认，则即使同步副本的数量低于最小值（例如，它可以与只有领导者一样低），也可以提交并消费该消息。

卡夫卡提供的保证是，承诺的消息不会丢失，只要至少有一个同步副本处于活动状态即可。

在短暂的故障转移期后，Kafka将在节点故障时保持可用状态，但在网络分区存在时可能无法保持可用状态。

### 复制日志：法定人数，ISR和状态机（哦，我的！）
卡夫卡分区的核心是一个复制日志。复制日志是分布式数据系统中最基本的原语之一，并且有许多方法可以实现它们。其他系统可以使用复制日志作为实现其他分布式系统状态的原语。
复制日志模拟按照一系列值（通常编号为日志条目0,1,2，...）的顺序达成一致的过程。有很多方法可以实现这一点，但最简单和最快的方法是选择提供给它的值的排序。只要领导者还活着，所有的追随者只需要复制价值观和领导者选择的命令。

当然，如果领导者没有失败，我们就不需要追随者！当领导者死亡时，我们需要从追随者中选择一个新的领导者。但追随者本身可能会落后或崩溃，所以我们必须确保我们选择最新的追随者。日志复制算法必须提供的基本保证是，如果我们告诉客户一个消息被提交，并且领导者失败，我们选择的新领导者也必须拥有这个消息。这产生了一个折衷：如果领导者在宣布承诺之前等待更多的追随者承认消息，那么将会有更多潜在的可选领导者。

如果您选择所需的确认数量以及必须比较的日志数量来选择领导者以确保重叠，那么这称为仲裁。

这种权衡的一种常见方法是对承诺决定和领导者选举使用多数票。这不是卡夫卡所做的，但我们无论如何都要探索它来理解这种权衡。假设我们有2f + 1个副本。如果f + 1副本必须在领导者声明提交之前接收到消息，并且如果我们通过从至少f + 1副本中选择具有最完整日志的跟随者来选择新领导者，则不超过f失败时，领导者保证拥有所有承诺的信息。这是因为在任何f + 1副本中，必须至少有一个副本包含所有提交的消息。该复制品的日志将是最完整的，因此将被选为新领导者。每种算法都必须处理许多其他细节（例如，精确定义什么使得日志更加完整，确保领导者失败期间的日志一致性或更改副本集中的服务器集），但我们现在将忽略这些细节。

这种多数票方式有一个非常好的属性：延迟仅取决于最快的服务器。也就是说，如果复制因子是三，则等待时间由较快的从属者而不是较慢的从属者确定。

这个系列有许多种算法，包括ZooKeeper的Zab，Raft和Viewstamped Replication。我们知道Kafka实际实施的最类似的学术出版物是微软的PacificA。

大多数投票的不利之处在于，没有多少失败让你没有可选领导。要容忍一次故障需要三份数据，而要容忍两次故障则需要五份数据。根据我们的经验，只有足够的冗余来容忍单个故障对于实际系统来说是不够的，但对于大容量数据问题而言，每写五次，磁盘空间需求量的5倍和吞吐量的1/5，都不太实际。这可能是为什么仲裁算法更常用于共享群集配置（如ZooKeeper）的原因，但对于主数据存储不太常见。例如在HDFS中，namenode的高可用性功能基于大多数投票日志，但这种更昂贵的方法不适用于数据本身。

卡夫卡采用稍微不同的方法来选择法定人数。 Kafka不是多数投票，而是动态地维护一组被引导到领导者的同步复制品（ISR）。只有这一组的成员才有资格当选领导。写入Kafka分区在所有同步副本收到写入之前不会被视为已提交。只要它发生变化，这个ISR集就会被持久化到ZooKeeper。正因为如此，ISR中的任何复制品都有资格当选为领导者。这是Kafka使用模式的重要因素，其中有很多分区并确保领导力平衡很重要。有了这个ISR模型和f + 1副本，Kafka主题可以容忍f故障而不会丢失承诺的消息。

对于我们希望处理的大多数用例，我们认为这种折衷是合理的。在实践中，为了容忍f失败，大多数投票和ISR方法都会在提交消息之前等待相同数量的副本进行确认（例如，为了在一次失败后生存下来，大多数法定人数需要三次副本和一次确认，并且ISR方法需要两个副本和一个确认）。没有最慢服务器的情况下提交的能力是多数投票方法的一个优点。但是，我们认为通过允许客户端选择是否阻止消息提交来改善它，并且由于所需复制因子较低而产生的额外吞吐量和磁盘空间是值得的。

另一个重要的设计区别是，Kafka不要求崩溃的节点在所有数据完好无损的情况下恢复。在这个空间中的复制算法依赖于存在“稳定存储”的情况并不少见，这种“稳定存储”在没有潜在的一致性违反的情况下在任何故障恢复场景中都不会丢失。这个假设有两个主要问题。首先，磁盘错误是我们在永久数据系统的实际操作中观察到的最常见的问题，并且它们通常不会使数据保持原样。其次，即使这不是问题，我们也不希望在每次写入时都要求使用fsync来保证一致性，因为这会将性能降低两到三个数量级。我们允许副本重新加入ISR的协议确保了在重新加入之前，即使丢失未刷新的数据，它也必须重新同步。

### 不洁的领导人选举：如果他们都死了会怎么样？
请注意，Kafka关于数据丢失的保证取决于至少一个副本保持同步。 如果复制分区的所有节点都死亡，则此保证不再成立。

然而，当所有副本死亡时，实际系统需要做一些合理的事情。 如果你不幸发生这种情况，重要的是要考虑会发生什么。 有两种行为可以实施：
1. 等待ISR中的复制品恢复生命并选择这个复制品作为领导者（希望它仍然拥有其所有数据）。
2. 选择第一个复制品（不一定在ISR中）作为领导者复活。

这是可用性和一致性之间的简单折衷。如果我们在ISR中等待副本，那么只要这些副本停机，我们将保持不可用状态。如果这些副本被毁坏或者他们的数据丢失了，那么我们会永久失效。另一方面，如果一个非同步副本恢复生机，并且我们允许它成为领导者，那么它的日志成为真实的来源，即使它不能保证每一个提交的消息都是如此。默认情况下，卡夫卡选择第二种策略，并赞成在ISR中的所有副本都已死亡时选择可能不一致的副本。可以使用配置属性unclean.leader.election.enable禁用此行为，以支持停机时间优于不一致情况的用例。

这种困境并不特定于卡夫卡。它存在于任何基于法定人数的计划中。例如，在大多数投票计划中，如果大多数服务器遭受永久性故障，那么您必须选择丢失100％的数据，或者通过将现有服务器上剩下的内容作为新的事实来源来破坏一致性。
### 可用性和耐久性保证
写给Kafka时，制作者可以选择是否等待消息被0,1或全部（-1）副本确认。 请注意，“所有副本确认”并不保证已分配副本的全套已收到该消息。 默认情况下，当acks = all时，只要所有当前的同步副本收到消息，确认就会发生。 例如，如果一个主题配置了只有两个副本并且一个失败（即，只有一个同步副本保留），那么指定acks = all的写入将会成功。 但是，如果剩余副本也失败，则这些写入操作可能会丢失。 虽然这确保了分区的最大可用性，但对于偏好耐用性而非可用性的一些用户而言，这种行为可能是不希望的。 因此，我们提供了两种主题级配置，可用于优先考虑消息的持久性：
1. 禁用不干净的领导者选举 - 如果所有副本都不可用，那么分区将保持不可用，直到最近的领导者再次变得可用。这有效地避免了消息丢失的风险。有关澄清，请参阅前面关于Unclean Leader Election的部分。
2. 指定最小的ISR大小 - 如果ISR的大小超过某个最小值，分区将只接受写入操作，以防止丢失仅写入单个副本的消息，随后该消息不可用。此设置仅在生产者使用acks = all并且保证该消息至少被许多同步副本确认时才会生效。此设置提供了一致性和可用性之间的折中。对于最小ISR大小的更高设置保证了更好的一致性，因为可以保证将消息写入更多副本，从而降低丢失概率。但是，它会降低可用性，因为如果同步副本的数量低于最小阈值，则分区将无法写入。

### 副本管理
以上关于复制日志的讨论确实只涉及单个日志，即一个主题分区。然而，Kafka集群将管理数百或数千个这些分区。我们试图以循环方式平衡集群内的分区，以避免在少数节点上集中高容量主题的所有分区。同样，我们试图平衡领导力，使每个节点成为其分区比例份额的领导者。

优化领导层选举过程也很重要，因为这是不可用的关键窗口。领导者选举的天真执行最终会针对该节点失败时托管的节点的每个分区运行一次选举。相反，我们选择其中一个经纪人作为“控制者”。该控制器检测代理级别的故障，并负责更改故障代理中所有受影响的分区的负责人。结果是我们可以将许多所需的领导变革通知批量化，这使得选举过程对于大量分区而言更便宜和更快。如果控制器失败，其中一个幸存的经纪人将成为新的控制者。

## 日志压缩
日志压缩可确保Kafka始终至少为单个主题分区的数据日志中的每个消息密钥保留最后已知的值。它解决用例和场景，例如在应用程序崩溃或系统故障后恢复状态，或者在运行维护期间重新启动应用程序后重新加载缓存。让我们更详细地介绍这些用例，然后描述压缩如何工作。

到目前为止，我们只描述了更简单的数据保留方法，其中旧日志数据在固定时间段后被丢弃或日志达到某个预定大小。这适用于时间事件数据，例如记录每个记录独立的地方。然而，重要的一类数据流是对键控，可变数据（例如对数据库表的更改）进行更改的日志。

我们来讨论一个这样的流的具体例子。假设我们有包含用户电子邮件地址的主题;每次用户更新其电子邮件地址时，我们都会使用其用户标识作为主键向此主题发送消息。现在说我们在id为123的用户的某个时间段内发送以下消息，每个消息对应于电子邮件地址的更改（其他id的消息都被省略）：
```
123 => bill@microsoft.com
        .
        .
        .
123 => bill@gatesfoundation.org
        .
        .
        .
123 => bill@gmail.com
```
日志压缩为我们提供了更细化的保留机制，因此我们保证至少保留每个主键的最新更新（例如bill@gmail.com）。 通过这样做，我们保证日志包含每个密钥的最终值的完整快照，而不仅仅是最近更改的密钥。 这意味着下游消费者可以从这个主题恢复自己的状态，而无需保留所有更改的完整日志。

我们先看几个有用的用例，然后看看它如何使用。
1. 数据库更改订阅。通常需要在多个数据系统中拥有一个数据集，而且这些系统中的一个常常是某种数据库（RDBMS或可能是一个新开发的键值存储）。例如，您可能有一个数据库，一个缓存，一个搜索集群和一个Hadoop集群。对数据库的每次更改都需要反映在缓存，搜索群集中，最终将反映在Hadoop中。在只处理实时更新的情况下，您只需要最近的日志。但是，如果您希望能够重新加载缓存或恢复失败的搜索节点，则可能需要完整的数据集。
2. 事件采购。这是一种应用程序设计风格，它将查询处理与应用程序设计协同定位，并使用更改日志作为应用程序的主存储。
3. 日志记录高可用性。执行本地计算的进程可以通过注销它对本地状态所做的更改来实现容错，以便另一个进程可以重新加载这些更改并在出现故障时继续进行。一个具体的例子是在流查询系统中处理计数，聚合和其他“按类”处理。 Samza是一个实时流处理框架，完全用于此目的。

在这些情况下，主要需要处理变更的实时馈送，但偶尔当机器崩溃或需要重新加载或重新处理数据时，需要执行全部加载。日志压缩允许将这两个用例从同一个支持主题中提取出来。本博客文章更详细地介绍了这种日志的使用方式。
总的想法很简单。如果我们拥有无限的日志保留时间，并且记录了上述情况下的每次更改，那么我们将在每次从第一次开始时就捕获系统的状态。使用这个完整的日志，我们可以通过重放日志中的前N个记录来恢复到任何时间点。对于更新单条记录多次的系统，这个假设的完整日志并不是非常实用，因为即使对于稳定的数据集，日志的增长也没有限制。抛弃旧更新的简单日志保留机制将限制空间，但日志不再是恢复当前状态的一种方式 - 现在从日志开始恢复不再重新创建当前状态，因为可能根本不捕获旧更新。

日志压缩是提供更细粒度的每记录保留的机制，而不是更粗粒度的基于时间的保留。这个想法是有选择地删除记录，我们有一个更新的更新与相同的主键。这样，日志保证至少有每个密钥的最后一个状态。

此保留策略可以按每个主题进行设置，因此单个群集可以拥有一些主题，其中保留是按大小或时间强制执行的，还有其他主题是通过压缩保留强制执行的主题。

此功能受LinkedIn最古老且最成功的基础架构之一 - 数据库更改日志缓存服务（称为Databus）的启发。与大多数日志结构存储系统不同，Kafka是为订阅而构建的，它组织用于快速线性读取和写入的数据。与Databus不同的是，Kafka充当真相存储库，因此即使在上游数据源不可重播的情况下也是如此。
### 日志压缩基础知识
这是一个高级图片，显示每个消息的偏移量的Kafka日志的逻辑结构。
![log_cleaner_anatomy.png](./log_cleaner_anatomy.png)

日志的头部与传统的Kafka日志相同。它具有密集的连续偏移量并保留所有消息。日志压缩添加了处理日志尾部的选项。上面的图片显示了压缩尾部的日志。请注意，日志尾部的消息会保留第一次写入时指定的原始偏移量 - 这些消息永远不会更改。还要注意，即使具有该偏移量的消息已被压缩，所有偏移仍然保留在日志中的有效位置;在这种情况下，该位置与日志中出现的下一个最高偏移量无法区分。例如，在上面的图片中，偏移量36,37和38都是等效位置，并且从这些偏移量中的任何偏移量开始的读取将返回从38开始的消息集合。

压缩也允许删除。包含密钥和空有效负载的消息将被视为从日志中删除。这个删除标记会导致任何有关该密钥的先前消息被删除（就像任何带有该密钥的新消息一样），但是删除标记是特殊的，因为它们自身会在一段时间后被清除出日志以释放空间。删除不再保留的时间点在上图中标记为“删除保留点”。

压缩是通过定期重新复制日志段在后台完成的。清洗不会阻止读取，并且可以通过限制使用不超过可配置数量的I / O吞吐量来避免影响生产者和消费者。压缩日志段的实际过程如下所示：
![](log_compaction.png)

### 日志压缩提供了什么保证？
日志压缩保证以下内容：
1. 任何一位处于日志头部的消费者都会看到写入的每条消息;这些消息将具有连续的偏移量。可以使用主题的min.compaction.lag.ms来保证消息在被压缩之前必须经过的最短时间长度。即它提供了每个消息将保留在（未压缩的）头部多长时间的下限。
2. 消息的排序始终保持不变。压缩不会重新排序消息，只是删除一些消息。
3. 消息的偏移不会改变。它是日志中位置的永久标识符。
4. 任何从日志开始进行的消费者将按照他们写入的顺序至少看到所有记录的最终状态。此外，只要用户在小于主题的delete.retention.ms设置（缺省值为24小时）的时间内到达日志头部，就会看到所有已删除记录的删除标记。换句话说：由于删除标记的删除与读取同时发生，因此消费者可能会错过删除标记，如果它滞后delete.retention.ms。

### 日志压实细节
日志压缩是由日志清理器处理的，后者是后台线程池，用于重新记录日志段文件，删除其日志头部出现的记录。 每个压缩机线程的工作原理如下：
1. 它选择日志头与日志尾比率最高的日志
2. 它为日志头部中的每个键创建最后偏移量的简明摘要
3. 它从头到尾复制日志中的日志，删除日志中稍后发生的键。 新的干净分段会立即交换到日志中，因此所需的额外磁盘空间只是一个额外的日志分段（而不是日志的完整副本）。
4. 日志头的总结基本上只是一个空间紧凑的散列表。 它每个条目恰好使用24个字节。 因此，使用8GB清理缓冲区时，一次清理迭代可以清理大约366GB的日志头（假设1k消息）。

### 配置日志清理器
日志清理器默认启用。 这将启动更干净的线程池。 要在特定主题上启用日志清理，您可以添加特定于日志的属性
```
log.cleanup.policy=compact
```
这可以在主题创建时或使用alter topic命令完成。

日志清理器可以配置为保留日志的未压缩“头部”的最小量。 这是通过设置压缩时间滞后来启用的。
```
log.cleaner.min.compaction.lag.ms
```

这可以用来防止比最小消息时间更新的消息受到压缩。 如果未设置，则除了最后一个分段（即当前正在写入的分段）之外，所有日志段都有资格进行压缩。 即使所有消息都比最小压缩时间滞后更早，活动段也不会被压缩。

[这里](https://kafka.apache.org/documentation.html#brokerconfigs)介绍更清洁的配置。

### 配额
Kafka集群可以对请求执行配额以控制客户端使用的代理资源。 卡夫卡经纪商可以为每组客户实施两类客户配额，分享配额：
1. 网络带宽配额定义了字节速率阈值（自0.9开始）
2. 请求速率配额将CPU利用率阈值定义为网络和I / O线程的百分比（自0.11开始）

#### 为什么配额是必要的？

生产者和消费者可能以非常高的速度生产/消费大量数据或产生请求，从而垄断经纪人资源，导致网络饱和，并且通常会对其他客户和经纪人本身产生影响。 拥有配额可以防范这些问题，并且在大型多租户群集中更重要，其中一小部分表现不佳的客户端可能会降低用户体验良好的用户体验。 事实上，在将Kafka作为服务运行时，甚至可以根据约定的合同强制执行API限制。

#### 客户端分组
Kafka客户端的身份是代表安全集群中经过身份验证的用户的用户主体。 在支持未经身份验证的客户端的群集中，用户主体是代理使用可配置的PrincipalBuilder选择的未经身份验证的用户的分组。 Client-id是由客户端应用程序选择的具有有意义名称的客户端的逻辑分组。 元组（user，client-id）定义了共享用户主体和客户机ID的安全逻辑组的客户机。

配额可以应用于（用户，客户端ID），用户或客户端组。 对于给定的连接，将应用与连接匹配的最具体的配额。 配额组的所有连接都共享为该组配置的配额。 例如，如果（user =“test-user”，client-id =“test-client”）的生产配额为10MB / sec，则在用户“test-user”的所有生产者实例中与客户端 - ID“测试客户端”。

#### 配额配置
可以为（用户，客户端ID），用户和客户端组定义配额配置。 可以在任何需要更高（或更低）配额的配额级别上覆盖默认配额。 该机制与每个主题日志配置覆盖类似。 用户和（用户，客户端ID）配额覆盖写入/ config / users下的ZooKeeper，客户端配额覆盖写入/ config / clients下。 这些覆盖被所有经纪人阅读，并立即生效。 这使我们可以更改配额，而无需执行整个群集的滚动重新启动。 详情请看这里。 每个组的默认配额也可以使用相同的机制动态更新。

配额配置的优先顺序是：
1. /config/users/<user>/clients/<client-id>
2. /config/users/<user>/clients/<default>
3. /config/users/<user>
4. /config/users/<default>/clients/<client-id>
5. /config/users/<default>/clients/<default>
6. /config/users/<default>
7. /config/clients/<client-id>
8. /config/clients/<default>

中间件属性（quota.producer.default，quota.consumer.default）也可用于为客户端组设置网络带宽配额的默认值。 这些属性已被弃用，并将在以后的版本中删除。 客户端ID的默认配额可以在Zookeeper中设置，类似于其他配额覆盖和默认值。

#### 网络带宽配额
网络带宽配额定义为每组共享配额的客户端的字节速率阈值。 默认情况下，每个唯一客户端组都会收到由群集配置的固定配额（以字节/秒为单位）。 此配额是以每个经纪人为基础定义的。 在客户端被限制之前，每个客户端组可以发布/获取每个代理的最大X字节/秒。

#### 请求率配额
请求速率配额定义为客户端可以在请求处理程序上使用的时间百分比处理器配额窗口中每个代理的I / O线程和网络线程。 n％的配额表示一个线程的n％，因此配额超出了（（num.io.threads + num.network.threads）* 100）％的总容量。 每个客户端组在受到限制之前，可以在配额窗口中的所有I / O和网络线程中使用最高达n％的总百分比。 由于为I / O和网络线程分配的线程数通常基于代理主机上可用的内核数，因此请求速率限额表示可由共享配额的每组客户端使用的CPU的总百分比。

#### 强制

默认情况下，每个唯一客户端组都会收到由群集配置的固定配额。此配额是以每个中间件为基础定义的。每个客户可以在受到限制之前利用每个代理的配额。我们决定为每个代理定义这些配额比每个客户端拥有固定的群集带宽好得多，因为这需要一种机制来在所有代理中共享客户端配额使用。这可能比配额实施本身更难获得！

中间件在检测到配额违规时如何反应？在我们的解决方案中，代理不会返回错误，而是尝试减慢超过配额的客户端速度。它计算将有罪客户置于其配额之下所需的延迟时间，并延迟当时的响应时间。这种方法使配额违反对客户端透明（客户端指标之外）。这也使他们不必执行任何特殊的退避和重试行为，这可能会变得棘手。事实上，糟糕的客户行为（无退避情况下的重试）可能会加剧配额试图解决的问题。

字节速率和线程利用率是在多个小窗口（例如每个窗口为1秒的30个窗口）上测量的，以便快速检测和纠正配额违规。通常，具有较大的测量窗口（例如每个30秒的10个窗口）导致大量的业务量突发，然后是长时间的延迟，这在用户体验方面不是很好。
